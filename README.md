# LLM Learning Journey

This repository documents my hands-on journey into **Large Language Models (LLMs)**.  
The goal is to build practical understanding through small, day-wise experiments covering model inference, fine-tuning, RAG pipelines, and multi-agent frameworks.

---

## ğŸ“… Progress

### Day 1 â€“ Getting Started with Hugging Face Pipelines
- Set up a simple text generation pipeline using Hugging Face (`transformers`).
- Explored decoding strategies:
  - **Greedy search**
  - **Sampling with temperature**
  - **Nucleus sampling (top-p)**
- Compared outputs of the same query under different decoding parameters.

---

## ğŸ”§ Tech Stack
- Python
- Hugging Face Transformers
- PyTorch
- Jupyter/Colab

---

## ğŸš€ Roadmap
- [ ] Day 2 â€“ Experiment with max_length vs max_new_tokens  
- [ ] Day 3 â€“ Fine-tuning a small model on custom data  
- [ ] Day 4 â€“ Building a simple Retrieval-Augmented Generation (RAG) pipeline  
- [ ] Day 5 â€“ Exploring multi-agent frameworks  
- [ ] Deployment experiments with Gradio/Flask  

---

## ğŸ“– References
- [Hugging Face Transformers Docs](https://huggingface.co/docs/transformers)  
- [Text Generation Strategies](https://huggingface.co/docs/transformers/main/en/generation_strategies)  

---

## ğŸ¤ Contributing
This repo is mainly for personal learning, but suggestions and improvements are welcome.  
Feel free to open issues or PRs!
