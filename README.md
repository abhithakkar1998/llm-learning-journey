# LLM Learning Journey

This repository documents my hands-on journey into **Large Language Models (LLMs)**.  
The goal is to build practical understanding through small, day-wise experiments covering model inference, fine-tuning, RAG pipelines, and multi-agent frameworks.

---

## 📅 Progress

### Day 1 – Getting Started with Hugging Face Pipelines
- Set up a simple text generation pipeline using Hugging Face (`transformers`).
- Explored decoding strategies:
  - **Greedy search**
  - **Sampling with temperature**
  - **Nucleus sampling (top-p)**
- Compared outputs of the same query under different decoding parameters.

---

## 🔧 Tech Stack
- Python
- Hugging Face Transformers
- PyTorch
- Jupyter/Colab

---

## 🚀 Roadmap
- [ ] Day 2 – Experiment with max_length vs max_new_tokens  
- [ ] Day 3 – Fine-tuning a small model on custom data  
- [ ] Day 4 – Building a simple Retrieval-Augmented Generation (RAG) pipeline  
- [ ] Day 5 – Exploring multi-agent frameworks  
- [ ] Deployment experiments with Gradio/Flask  

---

## 📖 References
- [Hugging Face Transformers Docs](https://huggingface.co/docs/transformers)  
- [Text Generation Strategies](https://huggingface.co/docs/transformers/main/en/generation_strategies)  

---

## 🤝 Contributing
This repo is mainly for personal learning, but suggestions and improvements are welcome.  
Feel free to open issues or PRs!
