{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry4yIxY2c60Q"
   },
   "source": [
    "# Day 4 – Semantic Search Mini-Project\n",
    "\n",
    "Building a **mini semantic search engine** using the concepts learned earlier:\n",
    "- Convert a list of sentences into **embeddings** using a Transformer model.\n",
    "- Store these embeddings in memory (can be extended to a database or FAISS later).\n",
    "- Take a user query, embed it, and compute **cosine similarity** with stored embeddings.\n",
    "- Retrieve and rank the most similar sentences.\n",
    "\n",
    "This is the foundation of modern **semantic search engines, chatbots, and recommendation systems**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries & Prepare Data\n",
    "#We'll use a small dataset of sentences for now.\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set a manual seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Sample dataset (can be replaced with your own text corpus)\n",
    "sentences = [\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"I love exploring natural language processing.\",\n",
    "    \"Transformers are the backbone of modern NLP.\",\n",
    "    \"Deep learning enables powerful AI systems.\",\n",
    "    \"I enjoy hiking in the mountains.\",\n",
    "    \"The weather today is sunny and bright.\",\n",
    "    \"Neural networks learn from data.\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sentences for semantic search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzvSFfiQdLtx"
   },
   "source": [
    "### Generating Sentence Embeddings\n",
    "\n",
    "To perform semantic search, each sentence must be converted into a numerical vector\n",
    "(embedding) that captures its meaning.\n",
    "\n",
    "Steps:\n",
    "1. Tokenize all sentences with the model's tokenizer.\n",
    "2. Pass tokens through a Transformer model (e.g., MiniLM, BERT).\n",
    "3. Apply **mean pooling** to get a single embedding per sentence.\n",
    "4. Store these embeddings for similarity search.\n",
    "\n",
    "We'll use a lightweight sentence-transformer model:\n",
    "`sentence-transformers/all-MiniLM-L6-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Tokenize sentences\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Mean pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return (token_embeddings * mask).sum(1) / mask.sum(1)\n",
    "\n",
    "sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "print(\"Generated sentence embeddings with shape:\", sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqb16NPsfQ_z"
   },
   "source": [
    "### Visualizing Sentence Embeddings\n",
    "\n",
    "High-dimensional embeddings (384 dimensions in MiniLM) are hard to interpret directly.\n",
    "We use **PCA (Principal Component Analysis)** to project them into 2D.\n",
    "\n",
    "Why?\n",
    "- Helps see if semantically similar sentences appear close together.\n",
    "- Useful for sanity check before search.\n",
    "\n",
    "Each point represents one sentence from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], color='blue', s=100)\n",
    "\n",
    "# Annotate sentences\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.annotate(f\"{i+1}\", (embeddings_2d[i, 0] + 0.02, embeddings_2d[i, 1]))\n",
    "\n",
    "plt.title(\"Sentence Embeddings Visualized with PCA\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klHGRJbAf71q"
   },
   "source": [
    "### Implementing Semantic Search\n",
    "\n",
    "How it works:\n",
    "1. **Embed the query** → Convert input text into a vector.\n",
    "2. **Compute cosine similarity** between query vector and all sentence embeddings.\n",
    "3. **Sort results** from most to least similar.\n",
    "4. **Return top N matches**.\n",
    "\n",
    "Cosine similarity is used because it focuses on the angle (semantic similarity) rather\n",
    "than raw distance (magnitude), making it robust for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=3):\n",
    "    # Tokenize and embed query\n",
    "    encoded_query = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_output = model(**encoded_query)\n",
    "        # ** is Python's argument unpacking operator for dictionaries.\n",
    "        # encoded_query is a dictionary returned by the tokenizer.\n",
    "        # The model's forward() method expects separate named arguments.\n",
    "        # **encoded_query automatically unpacks the dictionary into keyword arguments\n",
    "\n",
    "    query_embedding = mean_pooling(query_output, encoded_query['attention_mask'])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "    # Get top-k most similar sentences\n",
    "    top_indices = similarities.argsort()[::-1][:top_k]\n",
    "\n",
    "    print(f\"\\nQuery: {query}\\n\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"Score: {similarities[idx]:.4f} | Sentence: {sentences[idx]}\")\n",
    "\n",
    "# Example search\n",
    "semantic_search(\"I am interested in artificial intelligence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9ZsVBEBiNLe"
   },
   "source": [
    "### Using Wikipedia as a Text Source\n",
    "\n",
    "Instead of manually typing sentences, we'll fetch real-world content\n",
    "from Wikipedia using the `wikipedia` Python library.\n",
    "\n",
    "Why?\n",
    "- Provides diverse and relevant content for AI/ML/Data Science topics.\n",
    "- Makes our semantic search project more realistic and recruiter-friendly.\n",
    "- Easy to expand into a larger dataset later.\n",
    "\n",
    "Steps:\n",
    "1. Install and import `wikipedia`.\n",
    "2. Choose relevant topics (e.g., Artificial Intelligence, Machine Learning, Deep Learning).\n",
    "3. Extract text content.\n",
    "4. Clean and store it as sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the wikipedia library (if not already installed)\n",
    "!pip install wikipedia\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "topics = [\n",
    "    # Core AI/ML topics\n",
    "    'Artificial Intelligence',\n",
    "    'Machine Learning',\n",
    "    'Deep Learning',\n",
    "    'Neural Networks',\n",
    "    'Generative AI',\n",
    "    'Computer Vision',\n",
    "    'Large Language Model',\n",
    "    'Retrieval-augmented generation',\n",
    "    'Object Detection',\n",
    "    'Face Recognition',\n",
    "    'Natural Language Processing',\n",
    "    'Image Processing',\n",
    "\n",
    "    # Data Science & Analytics topics\n",
    "    'Data Science',\n",
    "    'Data Mining',\n",
    "    'Big Data',\n",
    "    'Data Analytics',\n",
    "    'Predictive Analytics',\n",
    "    'Statistical Modeling',\n",
    "    'Data Visualization',\n",
    "    'Exploratory Data Analysis',\n",
    "    'Data Cleaning',\n",
    "    'ETL (Extract Transform Load)',\n",
    "    'Business Intelligence',\n",
    "    'Data Warehousing',\n",
    "    'Feature Engineering',\n",
    "    'Time Series Analysis',\n",
    "    'Reinforcement Learning',\n",
    "    'Anomaly Detection',\n",
    "    'Data Governance',\n",
    "    'Data Ethics',\n",
    "    'Cloud Computing for Data Science'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your Wikipedia fetching loop:\n",
    "# corpus contains only successfully fetched summaries\n",
    "# successful_topics contains the corresponding topics (create this during fetch)\n",
    "\n",
    "successful_topics = []  # track which topics succeeded\n",
    "corpus = []             # your summaries\n",
    "\n",
    "for topic in topics:\n",
    "    try:\n",
    "        search_results = wikipedia.search(topic)\n",
    "        if not search_results:\n",
    "            print(f\"No results for: {topic}\")\n",
    "            continue\n",
    "        page = wikipedia.page(search_results[0])\n",
    "        corpus.append(page.summary)\n",
    "        successful_topics.append(page.title)  # store actual page title\n",
    "        print(f\"Fetched: {page.title}\")\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Skipped {topic} due to disambiguation: {e}\")\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Page not found for: {topic}\")\n",
    "\n",
    "print(f\"\\nTotal successful topics: {len(successful_topics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(successful_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0AJRumijmG7"
   },
   "source": [
    "### Generating Embeddings for Wikipedia Corpus\n",
    "\n",
    "Now that our dataset is populated with real-world AI/ML/Data Science content,\n",
    "we need to convert each article summary into an embedding.\n",
    "\n",
    "Steps:\n",
    "1. Tokenize each summary (batch mode for efficiency).\n",
    "2. Pass through the Transformer model.\n",
    "3. Apply mean pooling to get a single embedding per topic.\n",
    "4. Store them for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Wikipedia corpus\n",
    "inputs = tokenizer(corpus, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass through model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Mean pooling for sentence embeddings\n",
    "wiki_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "print(\"Generated embeddings for Wikipedia corpus:\", wiki_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzPuIX1Xj29v"
   },
   "source": [
    "### Visualizing Wikipedia Corpus Embeddings & Running Semantic Search\n",
    "\n",
    "Let's visualize the high-dimensional embeddings\n",
    "in 2D using PCA.\n",
    "\n",
    "Why?\n",
    "- To see how topics are distributed in semantic space.\n",
    "- Verify that related topics appear closer together (e.g., Machine Learning & Deep Learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure topics correspond only to successfully fetched summaries\n",
    "fetched_topics = [successful_topics[i] for i in range(len(corpus))]  # align with corpus length\n",
    "\n",
    "# Reduce dimensions to 2D\n",
    "pca = PCA(n_components=2)\n",
    "wiki_embeddings_2d = pca.fit_transform(wiki_embeddings)\n",
    "\n",
    "# Plot the topics\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(wiki_embeddings_2d[:, 0], wiki_embeddings_2d[:, 1], color='blue', s=80)\n",
    "\n",
    "# Annotate each point with the topic name (shortened for clarity)\n",
    "for i, topic in enumerate(fetched_topics):\n",
    "    plt.annotate(topic[:15] + \"...\", (wiki_embeddings_2d[i, 0] + 0.02, wiki_embeddings_2d[i, 1]))\n",
    "\n",
    "plt.title(\"Wikipedia AI/ML/Data Science Topics – PCA Visualization\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpZaX-EtkM-6"
   },
   "source": [
    "### Cosine Similarity Heatmap for Wikipedia Topics\n",
    "\n",
    "After generating embeddings for each topic, we can compute the pairwise\n",
    "cosine similarity between all topics.\n",
    "\n",
    "Why a heatmap?\n",
    "- Quickly shows which topics are most closely related.\n",
    "- Darker/brighter blocks indicate higher similarity.\n",
    "- Helps identify natural groupings (e.g., Machine Learning & Deep Learning\n",
    "are expected to be highly similar).\n",
    "\n",
    "We will use `sklearn.metrics.pairwise.cosine_similarity` to compute\n",
    "the similarity matrix and `seaborn` for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Compute cosine similarity matrix for embeddings\n",
    "similarity_matrix = cosine_similarity(wiki_embeddings)\n",
    "\n",
    "# Adjust topics list for fetched ones\n",
    "fetched_topics = [successful_topics[i] for i in range(len(corpus))]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_matrix, xticklabels=fetched_topics, yticklabels=fetched_topics,\n",
    "            cmap=\"coolwarm\", annot=False, fmt=\".2f\", square=True, cbar=True)\n",
    "\n",
    "plt.title(\"Cosine Similarity Heatmap – Wikipedia AI/ML/Data Science Topics\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Semantic Search\n",
    "\n",
    "def semantic_search_wikipedia(query, top_k=5):\n",
    "    # Tokenize and embed query\n",
    "    encoded_query = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_output = model(**encoded_query)\n",
    "    query_embedding = mean_pooling(query_output, encoded_query['attention_mask'])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, wiki_embeddings)[0]\n",
    "\n",
    "    # Get top-k most similar topics\n",
    "    top_indices = similarities.argsort()[::-1][:top_k]\n",
    "\n",
    "    print(f\"\\nQuery: {query}\\n\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"Score: {similarities[idx]:.4f} | Topic: {successful_topics[idx]}\")\n",
    "        print(f\"Summary: {corpus[idx][:300]}...\\n\")\n",
    "\n",
    "# Example search\n",
    "semantic_search_wikipedia(\"Explain the basics of computer vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOfhMcn07ssY"
   },
   "source": [
    "### Saving Cosine Similarity Matrix for Future Use\n",
    "\n",
    "To reuse the computed similarity matrix without recalculating embeddings each time,\n",
    "we will export it in two formats:\n",
    "\n",
    "1. **CSV** – Easy to open and inspect in Excel or Google Sheets.\n",
    "2. **JSON** – Convenient for programmatic use in APIs or web apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "folder_name = \"artifacts\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Save similarity matrix as CSV\n",
    "csv_path = os.path.join(folder_name, \"wikipedia_similarity_matrix.csv\")\n",
    "df_similarity.to_csv(csv_path)\n",
    "print(f\"Saved: {csv_path}\")\n",
    "\n",
    "# Save as JSON\n",
    "json_path = os.path.join(folder_name, \"wikipedia_similarity_matrix.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(similarity_json, f, indent=4)\n",
    "print(f\"Saved: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Wikipedia corpus as CSV\n",
    "corpus_df = pd.DataFrame({\"Topic\": successful_topics, \"Summary\": corpus})\n",
    "corpus_csv_path = os.path.join(folder_name, \"wikipedia_corpus.csv\")\n",
    "corpus_df.to_csv(corpus_csv_path, index=False)\n",
    "print(f\"Saved: {corpus_csv_path}\")\n",
    "\n",
    "# Save as JSON\n",
    "corpus_json = [{\"topic\": t, \"summary\": s} for t, s in zip(successful_topics, corpus)]\n",
    "corpus_json_path = os.path.join(folder_name, \"wikipedia_corpus.json\")\n",
    "with open(corpus_json_path, \"w\") as f:\n",
    "    json.dump(corpus_json, f, indent=4)\n",
    "print(f\"Saved: {corpus_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaKOpu0P9cbJ"
   },
   "source": [
    "### Semantic Search Function\n",
    "\n",
    "We will now create a simple semantic search function that:\n",
    "1. Takes a user query as input.\n",
    "2. Converts the query into an embedding using the same model as the corpus.\n",
    "3. Computes cosine similarity between the query embedding and each topic's embedding.\n",
    "4. Returns the top N most semantically similar topics with their scores.\n",
    "\n",
    "This is the core functionality of any semantic search engine before adding a UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def semantic_search(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform semantic search on the Wikipedia corpus.\n",
    "\n",
    "    Args:\n",
    "        query (str): User's search query.\n",
    "        top_k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: [(topic, similarity_score, summary), ...]\n",
    "    \"\"\"\n",
    "    # Tokenize the query\n",
    "    encoded_query = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate query embedding\n",
    "    with torch.no_grad():\n",
    "        query_output = model(**encoded_query)\n",
    "\n",
    "    query_embedding = mean_pooling(query_output, encoded_query['attention_mask'])\n",
    "\n",
    "    # Compute cosine similarity with all corpus embeddings\n",
    "    similarities = cosine_similarity(query_embedding, wiki_embeddings)[0]\n",
    "\n",
    "    # Sort by similarity score (descending)\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    # Collect results\n",
    "    results = [(successful_topics[i], similarities[i], corpus[i]) for i in top_indices]\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"What is used to make machines learn like humans?\"\n",
    "results = semantic_search(query, top_k=5)\n",
    "\n",
    "for rank, (topic, score, summary) in enumerate(results, start=1):\n",
    "    print(f\"{rank}. {topic} (score: {score:.4f})\")\n",
    "    print(f\"   Summary: {summary[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geFt0D2J9wnT"
   },
   "source": [
    "Visualizing Semantic Search Results\n",
    "\n",
    "To better understand which topics are most semantically related to the query,\n",
    "we can plot their similarity scores as a bar chart.\n",
    "\n",
    "- **X-axis:** Topics retrieved from the search.\n",
    "- **Y-axis:** Cosine similarity scores (0 to 1).\n",
    "- Higher bars mean greater semantic relevance to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_search_results(results, query):\n",
    "    \"\"\"\n",
    "    Visualize semantic search results as a bar chart.\n",
    "    Args:\n",
    "        results (list): Output of semantic_search (topic, score, summary)\n",
    "        query (str): The search query for title\n",
    "    \"\"\"\n",
    "    topics = [r[0] for r in results]\n",
    "    scores = [r[1] for r in results]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.barh(topics, scores, color='skyblue')\n",
    "    plt.xlabel(\"Cosine Similarity\")\n",
    "    plt.title(f\"Semantic Search Results for Query: \\\"{query}\\\"\")\n",
    "    plt.gca().invert_yaxis()  # Highest similarity at top\n",
    "\n",
    "    # Annotate scores on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                 f\"{score:.2f}\", va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the previous search results\n",
    "visualize_search_results(results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqBeghhT967r"
   },
   "source": [
    "## Building a Minimal Gradio Semantic Search UI\n",
    "\n",
    "We will create a lightweight user interface using Gradio to:\n",
    "1. Accept a user query.\n",
    "2. Perform semantic search on the Wikipedia corpus.\n",
    "3. Display the top N matching topics and their summaries.\n",
    "\n",
    "This is not the full deployment (which comes in Day 7),\n",
    "but a functional prototype for interactive testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def semantic_search_interface(query, top_k=5):\n",
    "    results = semantic_search(query, top_k=top_k)\n",
    "    display_text = \"\"\n",
    "    for rank, (topic, score, summary) in enumerate(results, start=1):\n",
    "        display_text += f\"### {rank}. {topic} (Score: {score:.4f})\\n\"\n",
    "        display_text += f\"{summary[:300]}...\\n\\n\"\n",
    "    return display_text\n",
    "\n",
    "# Define Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 🔍 Semantic Search – Wikipedia AI/ML Topics\")\n",
    "    query_input = gr.Textbox(label=\"Enter your statement related to AI/ML:\")\n",
    "    top_k_slider = gr.Slider(1, 10, value=5, step=1, label=\"Number of results\")\n",
    "    gr.Markdown(\"## Output: (NOTE - The output will be list of topics that \\\n",
    "    match the context of your input statement)\")\n",
    "    output_box = gr.Markdown()\n",
    "    search_button = gr.Button(\"Search\")\n",
    "\n",
    "    search_button.click(fn=semantic_search_interface,\n",
    "                        inputs=[query_input, top_k_slider],\n",
    "                        outputs=output_box)\n",
    "\n",
    "# Launch the app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-hB7p37AIfZ"
   },
   "source": [
    "## SUMMARY\n",
    "\n",
    "### What We Did\n",
    "- Built a **semantic search engine** using Wikipedia topics on AI/ML/Data Science.\n",
    "- Converted each topic's summary into **sentence embeddings** using a Transformer model.\n",
    "- Implemented a **search function** to retrieve most relevant topics based on cosine similarity.\n",
    "- Added **visualization** (bar chart) to interpret results.\n",
    "- Created a **minimal Gradio UI** for interactive semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "#### **Semantic Search**\n",
    "- Instead of keyword matching, we use **semantic meaning** to find relevant documents.\n",
    "- Queries and documents are both represented as **vectors in the same embedding space**.\n",
    "\n",
    "#### **Cosine Similarity**\n",
    "- Measures how close two vectors are, based on the angle between them.\n",
    "- **1.0 = identical meaning**, **0.0 = unrelated**.\n",
    "\n",
    "#### **Embeddings**\n",
    "- Generated using a pre-trained model (BERT-based in this case).\n",
    "- Each document (Wikipedia summary) → 768-dimensional vector.\n",
    "\n",
    "#### **Visualization**\n",
    "- Applied **PCA (Principal Component Analysis)** to reduce embeddings from 768D → 2D.\n",
    "- Plotted topics to observe **semantic clusters**.\n",
    "\n",
    "#### **Gradio Interface**\n",
    "- Built a minimal UI to:\n",
    "  - Input query\n",
    "  - Select number of results (`top_k`)\n",
    "  - Display ranked results with summaries\n",
    "\n",
    "---\n",
    "\n",
    "### Key Functions, Modules & Parameters\n",
    "- **transformers**\n",
    "  - `AutoTokenizer`, `AutoModel` for encoding text\n",
    "- **torch**\n",
    "  - `torch.no_grad()` for inference efficiency\n",
    "  - `manual_seed()` for reproducibility\n",
    "- **sklearn**\n",
    "  - `cosine_similarity()` for semantic relevance\n",
    "  - `PCA()` for visualization\n",
    "- **gradio**\n",
    "  - `Blocks()`, `Textbox()`, `Slider()`, `Markdown()`, `Button()` for the UI\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "- **Cosine Similarity Matrix** saved as:\n",
    "  - `artifacts/wikipedia_similarity_matrix.csv`\n",
    "  - `artifacts/wikipedia_similarity_matrix.json`\n",
    "- **Corpus (topics + summaries)** saved as:\n",
    "  - `artifacts/wikipedia_corpus.csv`\n",
    "  - `artifacts/wikipedia_corpus.json`\n",
    "- **Interactive Gradio App** for semantic search\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "- Demonstrates the foundation of **intelligent search engines** (e.g., Google, ChatGPT memory retrieval, document search).\n",
    "- Shows how to move from **raw text → embeddings → search interface**.\n",
    "- Prepares the ground for:\n",
    "  - **Day 5: Fine-tuning** (LoRA/PEFT)\n",
    "  - **Day 6: RAG (Retrieval-Augmented Generation)**\n",
    "  - **Day 7: Deployment with a polished UI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
