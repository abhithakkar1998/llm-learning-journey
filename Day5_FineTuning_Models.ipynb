{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WwWJD4W1yex"
   },
   "source": [
    "### What is Fine-Tuning?\n",
    "\n",
    "Fine-tuning is the process of taking a **pre-trained model** (already trained on a massive corpus)\n",
    "and adapting it to a **specific task or domain**.\n",
    "\n",
    "- **Full Fine-Tuning:** Update **all model parameters** (very compute-heavy, large storage).\n",
    "- **Parameter-Efficient Fine-Tuning (PEFT):** Only update **a few extra parameters** while keeping the base model frozen.\n",
    "\n",
    "#### Why PEFT?\n",
    "- Uses **~1–5% of the parameters** instead of 100%.\n",
    "- Faster, cheaper, and Colab-friendly.\n",
    "- Popular method: **LoRA (Low-Rank Adaptation)** – injects small trainable matrices into model layers.\n",
    "\n",
    "We'll use **Hugging Face PEFT library** to implement LoRA in just a few lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Oq48-nP2PCT"
   },
   "source": [
    "### What is LoRA (Low-Rank Adaptation)?\n",
    "\n",
    "LoRA is a **parameter-efficient fine-tuning (PEFT)** method that lets us adapt a large language model\n",
    "to a new task **without modifying most of its original weights**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Analogy – Adding Sticky Notes to a Book**\n",
    "- Imagine you have a **500-page textbook (pre-trained model)**.\n",
    "- Instead of rewriting the entire book (full fine-tuning), you just **add sticky notes in key pages**\n",
    "  to highlight changes or corrections (LoRA fine-tuning).\n",
    "- The original book remains **unchanged**, but when you read it with the sticky notes,\n",
    "  you get the **updated knowledge**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How LoRA Works?**\n",
    "- In standard transformers, large **weight matrices (W)** exist in attention layers.\n",
    "- LoRA adds **two small matrices (A & B)** of much lower rank (`r` ≪ full size).\n",
    "- During fine-tuning:\n",
    "  - The original weights stay **frozen**.\n",
    "  - Only **A & B are trained**.\n",
    "  - Final weight = `W + A×B`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Benefits of LoRA**\n",
    "- **Lightweight:** Only a few million parameters trained instead of billions.\n",
    "- **Composable:** Multiple LoRA adapters can be merged or switched on demand.\n",
    "- **Fast:** Works even on CPU or free-tier Colab with small models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7_crEga2UYH"
   },
   "source": [
    "### Environment Setup for LoRA Fine-Tuning\n",
    "\n",
    "We'll need:\n",
    "- **transformers**: to load pre-trained models and tokenizers.\n",
    "- **peft**: the Hugging Face library that implements LoRA and other parameter-efficient methods.\n",
    "- **datasets**: (optional) to easily load sample datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyflLcmq2eYA"
   },
   "source": [
    "### Creating a Tiny Dataset for Fine-Tuning\n",
    "\n",
    "We'll create a **toy sentiment classification dataset** with only two labels:\n",
    "- **Positive (1)**\n",
    "- **Negative (0)**\n",
    "\n",
    "This is intentionally tiny (just a few samples) to:\n",
    "- Keep training fast (works on free-tier Colab).\n",
    "- Focus on understanding **LoRA/PEFT workflow**, not achieving high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Few-shot dataset\n",
    "texts = [\n",
    "    \"I love this product! It's amazing.\",      # Positive\n",
    "    \"Absolutely fantastic service.\",           # Positive\n",
    "    \"This is the worst experience ever.\",      # Negative\n",
    "    \"I hate how slow this is.\",                # Negative\n",
    "    \"Pretty good, I might recommend it.\",      # Positive\n",
    "    \"Terrible! Would not buy again.\"           # Negative\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 0, 1, 0]\n",
    "\n",
    "# Create a Hugging Face Dataset object\n",
    "tiny_dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# Quick preview\n",
    "tiny_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZS1lixnq4HcG"
   },
   "source": [
    "### Tokenization & Preprocessing\n",
    "\n",
    "Transformers cannot directly read raw text — they need:\n",
    "- **input_ids**: numerical IDs for each token.\n",
    "- **attention_mask**: tells the model which tokens are padding (0) vs real (1).\n",
    "- **labels**: our target values (0 or 1 for sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for distilbert-base-uncased\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = tiny_dataset.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Keep only the necessary columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# Rename label to labels (expected by Trainer)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAYDqQhD4iwV"
   },
   "source": [
    "How the above code works:\n",
    "- First we load the vocabulary and tokenization rules for distilbert-base-uncased.\n",
    "- The `tokenize_batch` function takes a batch of text strings (batch[\"text\"]) and:\n",
    "  - Padding: Ensures each sentence is same length (here: 32 tokens max).\n",
    "  - Truncation: If a sentence is too long, it cuts off after 32 tokens.\n",
    "  - Returns: input_ids and attention_mask.\n",
    "  - Example:\n",
    "    - Input: \"I love this product!\"\n",
    "    - Output:\n",
    "    \n",
    "      input_ids: [101, 1045, 2293, 2023, 4031, 999, 102, 0, 0, ...]\n",
    "      \n",
    "      attention_mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, ...]\n",
    "- Applying Tokenization to the Dataset\n",
    "  - map() applies our function to every entry in the dataset.\n",
    "  - batched=True means it processes multiple samples at once (faster).\n",
    "- Remove the raw text column since the model won’t use it.\n",
    "- Renaming Labels: Hugging Face Trainer expects the label column to be named labels, not label\n",
    "\n",
    "---\n",
    "\n",
    "Mini Example Walkthrough:\n",
    "\n",
    "- Take this one sample:\n",
    "  - Text: \"I love this product!\"\n",
    "  - Label: 1\n",
    "- After tokenization (max_length=10 for simplicity):\n",
    "  - input_ids:      [101, 1045, 2293, 2023, 4031, 999, 102, 0, 0, 0]\n",
    "  - attention_mask: [1,   1,    1,    1,    1,   1,   1,  0, 0, 0]\n",
    "  - labels:         1\n",
    "\n",
    "Now the model has a numerical format it can work with.\n",
    "\n",
    "For a more detailed explanation watch these -\n",
    "- https://www.youtube.com/watch?v=KEv-F5UkhxU\n",
    "- https://www.youtube.com/watch?v=t1caDsMzWBk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgG6BiHhNQIW"
   },
   "source": [
    "### Loading the Base Model\n",
    "\n",
    "We'll use:\n",
    "- `AutoModelForSequenceClassification` — a pre-trained DistilBERT model\n",
    "  with a classification head suitable for binary classification.\n",
    "- Since we're doing LoRA, the **main transformer weights will stay frozen**\n",
    "  and only LoRA adapter layers will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-trained DistilBERT for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2  # Binary classification: Positive / Negative\n",
    ")\n",
    "\n",
    "# Quick check of model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHBTTawaOehO"
   },
   "source": [
    "### Attach LoRA Adapters?\n",
    "\n",
    "- **LoRA (Low-Rank Adaptation)** adds small trainable matrices inside the\n",
    "  `attention` layers (specifically in `q_lin` and `v_lin` for DistilBERT).\n",
    "  - Query (Q): Represented by q_lin, this vector is used to query the other vectors.\n",
    "  - Key (K): Represented by k_lin, this vector is used to determine how relevant other vectors are to the query.\n",
    "  - Value (V): Represented by v_lin, this vector contains the information to be extracted from the sequence.\n",
    "- The **original weights remain frozen**, while LoRA layers learn the task.\n",
    "- This makes fine-tuning *fast, cheap, and memory-efficient*.\n",
    "\n",
    "Key parameters:\n",
    "- `r`: Rank of LoRA matrices (controls size/complexity).\n",
    "- `lora_alpha`: Scaling factor (influences the effective learning rate for LoRA).\n",
    "- `target_modules`: Layers where LoRA will be applied (`q_lin`, `v_lin` for DistilBERT attention).\n",
    "- `lora_dropout`: Dropout applied within LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                      # Rank of the low-rank adapters\n",
    "    lora_alpha=32,            # Scaling factor for adaptation\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # Target the attention projection layers\n",
    "    lora_dropout=0.1,         # Dropout within LoRA layers\n",
    "    bias=\"none\",              # Don't modify biases\n",
    "    task_type=\"SEQ_CLS\"       # Task type: Sequence Classification\n",
    ")\n",
    "\n",
    "# Apply LoRA to the DistilBERT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print number of trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zYGmEJhQn9d"
   },
   "source": [
    "### Trainer Setup for LoRA Fine-Tuning\n",
    "\n",
    "We'll use Hugging Face's `Trainer` API:\n",
    "- Handles training loop, evaluation, and saving the model automatically.\n",
    "- Uses our tokenized dataset, LoRA-enabled model, and standard training arguments.\n",
    "\n",
    "Key components:\n",
    "- **TrainingArguments**: controls epochs, learning rate, logging, saving.\n",
    "- **Trainer**: handles forward pass, backpropagation, optimizer, evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample toy dataset\n",
    "texts = [\n",
    "    \"I love this product, it works great!\",\n",
    "    \"Absolutely fantastic experience, very satisfied.\",\n",
    "    \"Worst purchase ever, totally disappointed.\",\n",
    "    \"I will never buy this again.\",\n",
    "    \"Amazing quality and fast delivery.\",\n",
    "    \"Horrible support, waste of money.\",\n",
    "    \"Great value for the price.\",\n",
    "    \"Terrible, completely useless.\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_data = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"  # Keep tensors for PyTorch\n",
    ")\n",
    "\n",
    "# Add labels to the tokenized data\n",
    "tokenized_data[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose `tokenized_data` is a dictionary containing input_ids, attention_mask, labels\n",
    "# We'll split into train and validation sets for this toy dataset\n",
    "\n",
    "train_data, val_data = train_test_split(list(zip(tokenized_data['input_ids'],\n",
    "                                                 tokenized_data['attention_mask'],\n",
    "                                                 tokenized_data['labels'])),\n",
    "                                        test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [x[0] for x in train_data],\n",
    "    \"attention_mask\": [x[1] for x in train_data],\n",
    "    \"labels\": [x[2] for x in train_data]\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [x[0] for x in val_data],\n",
    "    \"attention_mask\": [x[1] for x in val_data],\n",
    "    \"labels\": [x[2] for x in val_data]\n",
    "})\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetune_output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"  # Disable wandb or other integrations for now\n",
    ")\n",
    "\n",
    "# Define a compute_metrics function\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3HMqfC19TrR"
   },
   "source": [
    "### Inference on New Text After Fine-Tuning\n",
    "\n",
    "After fine-tuning, we want to check:\n",
    "- How well the model generalizes to unseen data.\n",
    "- Whether it correctly predicts the sentiment for new sentences.\n",
    "\n",
    "We'll:\n",
    "1. Tokenize new unseen sentences.\n",
    "2. Run them through the fine-tuned model.\n",
    "3. Interpret the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example unseen sentences\n",
    "new_sentences = [\n",
    "    \"This product exceeded my expectations!\",\n",
    "    \"It was a total waste of money.\",\n",
    "    \"Delivery was quick but the quality is average.\"\n",
    "]\n",
    "\n",
    "# Tokenize new data\n",
    "new_inputs = tokenizer(\n",
    "    new_sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Move to same device as model (important for Colab/CPU)\n",
    "new_inputs = {k: v.to(model.device) for k, v in new_inputs.items()}\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**new_inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "# Interpret predictions\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "for sentence, pred in zip(new_sentences, predictions):\n",
    "    print(f\"Text: {sentence}\")\n",
    "    print(f\"Predicted Sentiment: {label_map[pred]}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABLkuJNW9dDp"
   },
   "source": [
    "What’s Happening Here?\n",
    "\n",
    "- Tokenization: Converts each new sentence into token IDs and masks.\n",
    "\n",
    "- Model Inference: Runs forward pass through DistilBERT + LoRA adapters.\n",
    "\n",
    "- Prediction: Uses torch.argmax() to pick the most likely label.\n",
    "\n",
    "- Interpretation: Maps the numeric label to \"Positive\" or \"Negative\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWuzPQ_J95xy"
   },
   "source": [
    "#### Parameter-Wise Inside DistilBERT\n",
    "\n",
    "- Attention layers have query (Wq) and value (Wv) projection matrices:\n",
    "- Each is a large d_model × d_model weight matrix (e.g., 768×768).\n",
    "- LoRA adds two small matrices (A & B) per adapted layer:\n",
    "  \n",
    "  W' = W + ΔW\n",
    "\n",
    "  ΔW = A × B   # Low-rank decomposition\n",
    "  - A: size [d_model, r]\n",
    "  - B: size [r, d_model]\n",
    "  - Rank r is small (e.g., 8), so trainable params ≪ original.\n",
    "\n",
    "- Original DistilBERT weights remain frozen.\n",
    "- No update to embeddings or FFN weights.\n",
    "- The final classification layer (Linear(768 → 2)) is also trainable.\n",
    "- This is because we need to adapt to the new downstream task.\n",
    "- `peft.get_peft_model()` injected LoRA layers into the DistilBERT attention blocks.\n",
    "- We froze all original weights, trained LoRA adapters + classifier head only.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aAfCqVg-5Zf"
   },
   "source": [
    "## **Summary**\n",
    "### **What We Learned**\n",
    "- Fine-tuning Large Language Models (LLMs) can be **expensive and resource-heavy** if we update all parameters.\n",
    "- **LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning (PEFT) technique that:\n",
    "  - Freezes original model weights.\n",
    "  - Adds small trainable matrices (low-rank adapters) to attention layers.\n",
    "  - Drastically reduces the number of trainable parameters while maintaining good performance.\n",
    "\n",
    "### **How We Did It**\n",
    "1. **Prepared a Toy Sentiment Dataset**\n",
    "   - Used simple labeled sentences: Positive/Negative sentiment.\n",
    "   - Tokenized using `AutoTokenizer` with `padding`, `truncation`, and `max_length`.\n",
    "\n",
    "2. **Loaded Base Model**\n",
    "   - `DistilBERT (distilbert-base-uncased)` with a sequence classification head.\n",
    "   - Observed that classification layers were newly initialized.\n",
    "\n",
    "3. **Attached LoRA Adapters**\n",
    "   - Applied using `peft.get_peft_model()` on attention layers (`query` and `value` projections).\n",
    "   - Only LoRA parameters and classifier head were made trainable.\n",
    "\n",
    "4. **Fine-Tuned with Trainer**\n",
    "   - Split dataset into training and validation sets (80/20).\n",
    "   - Used `TrainingArguments` with:\n",
    "     - `eval_strategy=\"epoch\"`\n",
    "     - `learning_rate=5e-4`\n",
    "     - `num_train_epochs=3`\n",
    "     - `load_best_model_at_end=True`\n",
    "   - Logged **accuracy** and **F1-score**.\n",
    "\n",
    "5. **Inference on New Text**\n",
    "   - Tokenized unseen sentences.\n",
    "   - Predicted sentiment (`Positive` or `Negative`) using fine-tuned model.\n",
    "\n",
    "### **Key Concepts & Parameters**\n",
    "- **LoRA Rank (`r`)**: Controls the size of trainable matrices (small r = fewer parameters).\n",
    "- **Freezing Base Weights**: Prevents catastrophic forgetting and reduces memory usage.\n",
    "- **Classification Head**: Always retrained for downstream task.\n",
    "\n",
    "### **Modules Used**\n",
    "- `transformers`: AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "- `peft`: LoRA integration\n",
    "- `datasets`: For train/validation split\n",
    "- `torch`: Model inference and device management\n",
    "- `sklearn`: Accuracy and F1 evaluation metrics\n",
    "\n",
    "### **Why LoRA?**\n",
    "- **Full fine-tuning**: Updates all parameters (~66M for DistilBERT).\n",
    "- **LoRA fine-tuning**: Updates only a small fraction (~hundreds of thousands).\n",
    "- Achieves nearly similar accuracy while being faster and lightweight.\n",
    "\n",
    "### **Outcome**\n",
    "- Successfully fine-tuned a DistilBERT model using LoRA adapters.\n",
    "- Reduced compute requirements for training.\n",
    "- Validated the model on unseen data and achieved reliable predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
