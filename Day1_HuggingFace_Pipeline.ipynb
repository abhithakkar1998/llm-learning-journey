{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abhithakkar1998/llm-learning-journey/blob/main/Day1_HuggingFace_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers datasets huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’ll use Hugging Face’s free model google/flan-t5-base (a small instruction-tuned model).\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a small model for text generation\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# Test query\n",
    "query = \"Explain neural networks in simple words.\"\n",
    "response = generator(query, max_length=100, do_sample=True)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Response:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K641LXvDNJoH"
   },
   "source": [
    "###Let’s open the black box a bit.###\n",
    "\n",
    "When you ran the above code it looked like magic. But under the hood, it’s really 3 steps:\n",
    "\n",
    "1. Tokenization (Text → Numbers)\n",
    "  * LLMs don’t “understand” words directly.\n",
    "  * Every word/piece of a word is mapped to a number (token ID).\n",
    "  * Example: \"Explain neural networks in simple words.\"<br/>\n",
    "  ↓<br/>\n",
    "  [\"Explain\", \" neural\", \" networks\", \" in\", \" simple\", \" words\", \".\"]<br/>\n",
    "  ↓<br/>\n",
    "  [1234, 5678, 91011, 2345, 6789, 1112, 42]<br/>\n",
    "  These IDs are like a language encoding that the model can process.\n",
    "\n",
    "2. Model Inference (Numbers → Numbers)\n",
    "  * The token IDs are fed into a neural network (a giant sequence model like T5).\n",
    "  * The model predicts the probability of the next token.\n",
    "  * Example: If the current text is \"Neural networks are\", the model might predict:<br/>\n",
    "  \"powerful\" with 0.4 probability<br/>\n",
    "  \"complex\" with 0.3 probability<br/>\n",
    "  \"used\" with 0.2 probability<br/>\n",
    "  ....<br/>\n",
    "\n",
    "3. Decoding (Numbers → Text)\n",
    "  * The model keeps predicting the next token, one step at a time, until it finishes.\n",
    "  * Strategies for decoding:\n",
    "    * Greedy: Always pick the highest probability (may sound robotic).\n",
    "    * Sampling: Randomly pick based on probability distribution (adds variety).\n",
    "    * Beam search: Explore multiple possible continuations, then choose the best one.\n",
    "  * Finally, the generated token IDs are mapped back into words: <br/>\n",
    "  [1234, 5678, 91011, 2345, 6789, 1112, 42] -> \"Neural networks are powerful models that mimic the brain...\"\n",
    "\n",
    "###Pipeline = Wrapper###\n",
    "\n",
    "The pipeline function in Hugging Face just wraps all 3 steps:\n",
    "* Tokenize input.\n",
    "* Run the model forward pass.\n",
    "* Decode output.\n",
    "\n",
    "So you can focus on the task (“summarize”, “translate”, “answer question”) instead of manually handling tokens.\n",
    "\n",
    "###Why is the output vague?###\n",
    "* Model size & capability: flan-t5-base is only ~250M parameters → designed for small tasks. It’s instruction-tuned, but it tends to produce short answers.\n",
    "* Generation parameters: By default, the pipeline doesn’t force long responses.\n",
    "* Decoding strategy: Default decoding may lean towards very safe answers. Adding sampling or beam search helps produce richer outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCTL3Jn1POq1"
   },
   "source": [
    "###How to Improve the Output###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Increase output length + use sampling\n",
    "response = generator(\n",
    "    query,\n",
    "    max_new_tokens=100,       # allow up to 100 new tokens\n",
    "    do_sample=True,           # add randomness\n",
    "    temperature=0.7,          # controls creativity (0.7 is a good balance)\n",
    "    top_p=0.9                 # nucleus sampling\n",
    ")\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyXr-x-XgWM4"
   },
   "source": [
    "* max_new_tokens → Limits how many words/tokens the model can generate in its response.\n",
    "* do_sample → Enables randomness instead of always picking the most likely next word.\n",
    "* temperature → Controls how much creativity vs. predictability is allowed in generation.\n",
    "* top_p → Restricts choices to only the most likely words until a probability threshold is covered (nucleus sampling).\n",
    "\n",
    "For more details -\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/pipelines\n",
    "https://huggingface.co/docs/transformers/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a slightly larger model if Colab allows\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "query = \"Explain neural networks in simple words.\"\n",
    "response = generator(\n",
    "    query,\n",
    "    max_new_tokens=100,       # allow up to 100 new tokens\n",
    "    do_sample=True,           # add randomness\n",
    "    temperature=0.7,          # controls creativity (0.7 is a good balance)\n",
    "    top_p=0.9                 # nucleus sampling\n",
    ")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How the same query changes when we adjust temperature and top_p\n",
    "\n",
    "# 1. Greedy (deterministic, no sampling)\n",
    "resp_greedy = generator(query, max_new_tokens=60, do_sample=False)\n",
    "\n",
    "# 2. Sampling with low temperature (more focused)\n",
    "resp_temp_low = generator(query, max_new_tokens=60, do_sample=True, temperature=0.3, top_p=0.9)\n",
    "\n",
    "# 3. Sampling with medium temperature (balanced)\n",
    "resp_temp_mid = generator(query, max_new_tokens=60, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "\n",
    "# 4. Sampling with high temperature (creative, risk of nonsense)\n",
    "resp_temp_high = generator(query, max_new_tokens=60, do_sample=True, temperature=1.2, top_p=0.9)\n",
    "\n",
    "print(\"\\n=== Greedy ===\")\n",
    "print(resp_greedy[0]['generated_text'])\n",
    "\n",
    "print(\"\\n=== Low Temperature (0.3) ===\")\n",
    "print(resp_temp_low[0]['generated_text'])\n",
    "\n",
    "print(\"\\n=== Medium Temperature (0.7) ===\")\n",
    "print(resp_temp_mid[0]['generated_text'])\n",
    "\n",
    "print(\"\\n=== High Temperature (1.2) ===\")\n",
    "print(resp_temp_high[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LChnQS4ohXit"
   },
   "source": [
    "Why the first three are identical?\n",
    "\n",
    "* Greedy (do_sample=False): Always picks the single most likely next token.\n",
    "* Sampling (do_sample=True): Picks randomly from the probability distribution of possible tokens.\n",
    "  * But… if one token has extremely high probability (say 0.98 or 0.99), then even with sampling and low/medium temperature, the model almost always picks that token. The result looks the same as greedy.\n",
    "\n",
    "* Why does temperature not change it much?\n",
    "  * Temperature scaling reshapes the probability distribution.\n",
    "  * Low temp (0.3) → makes the top choice even sharper, so variation disappears.\n",
    "  * Medium temp (0.7) → smooths a little, but if the top token is already dominant, it still wins.\n",
    "  * High temp (1.2) → finally flattens the distribution enough that second/third best tokens get picked, giving variation\n",
    "\n",
    "* What’s happening here specifically\n",
    "  * At the very first step of generation:\n",
    "    * The token \"a\" (start of \"a neural network...\") is overwhelmingly the top candidate.\n",
    "    * Temp 0.3 and 0.7 don’t change that — \"a\" is still chosen.\n",
    "    * Only at higher temperature (1.2) do alternative beginnings like \"Neural\" get some chance.\n",
    "\n",
    "SUMMARY: The first three are the same because the model is extremely confident about the first token and temperature 0.3–0.7 isn’t enough to shake it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
