{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLdmmZnbCi-z"
   },
   "source": [
    "# Day 2 — Tokenization (and what actually goes into the model)\n",
    "\n",
    "**Goal:** Understand what happens *before* an LLM produces text: tokenization → IDs → attention masks → back to text.\n",
    "\n",
    "**You will learn:**\n",
    "- What a *tokenizer* does (BERT-style WordPiece vs GPT-2 BPE).\n",
    "- How strings become **token IDs** and how IDs decode back to text.\n",
    "- What **special tokens** (`[CLS]`, `[SEP]`) and **attention masks** are.\n",
    "- (Next steps) How token IDs become **embeddings** the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Import minimal deps\n",
    "!pip -q install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HX384mysFWXL"
   },
   "source": [
    "We import PyTorch (torch) because Hugging Face models are implemented using it by default.\n",
    "This allows us to:\n",
    "\n",
    "Run computations on CPU/GPU\n",
    "\n",
    "Manage tensors and model weights\n",
    "\n",
    "Control device placement (e.g., cuda for GPU)\n",
    "\n",
    "Let’s quickly test whether GPU support is available in our Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg9HdkY1Crud"
   },
   "source": [
    "## A. Tokenization with BERT (WordPiece)\n",
    "\n",
    "- BERT uses **WordPiece**: splits uncommon words into subwords with the `##` prefix.\n",
    "- The tokenizer turns text → **token strings** → **token IDs**.\n",
    "- We also get an **attention mask**: 1 = real token, 0 = padding (for batching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Transformers are amazing! Neural networks learn patterns from data.\"\n",
    "\n",
    "tok_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "enc = tok_bert(\n",
    "    text,\n",
    "    add_special_tokens=True,     # add [CLS] ... [SEP]\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Input text:\\n\", text, \"\\n\")\n",
    "print(\"Token strings:\\n\", tok_bert.convert_ids_to_tokens(enc[\"input_ids\"][0]))\n",
    "print(\"\\nToken IDs:\\n\", enc[\"input_ids\"][0].tolist())\n",
    "print(\"\\nAttention mask:\\n\", enc[\"attention_mask\"][0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UILvD3bNEOSa"
   },
   "source": [
    "## B. Decoding back to text & special tokens\n",
    "\n",
    "- **[CLS]**: classification token at the start (BERT uses it as a sentence summary).\n",
    "- **[SEP]**: separator token (end of sentence; also used between sentence pairs).\n",
    "- **Decoding** reconstructs text from token IDs (not always identical spacing/punctuation, but equivalent).\n",
    "\n",
    "We’ll also map tokens ↔ IDs to see the vocabulary lookup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = enc[\"input_ids\"][0]\n",
    "decoded = tok_bert.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded text (no specials):\\n\", decoded, \"\\n\")\n",
    "\n",
    "# Token ↔ ID table (compact)\n",
    "tokens = tok_bert.convert_ids_to_tokens(ids)\n",
    "pairs = list(zip(tokens, ids.tolist()))\n",
    "for t, i in pairs[:20]:\n",
    "    print(f\"{t:>15}  -> {i}\")\n",
    "if len(pairs) > 20:\n",
    "    print(\"... (truncated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pg9vQjGEoFk"
   },
   "source": [
    "## C. Compare with GPT-2 (Byte-Pair Encoding, no pad token)\n",
    "\n",
    "- GPT-2 uses **BPE** (slightly different subword scheme).\n",
    "- It has **no PAD token** by default (so we usually don’t batch with padding unless we set one).\n",
    "- Notice how the token strings differ and there are **no `[CLS]`/`[SEP]`** special tokens by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# GPT-2 has no pad token by default; set one if needed for batching later\n",
    "tok_gpt2.pad_token = tok_gpt2.eos_token\n",
    "\n",
    "enc_gpt2 = tok_gpt2(\n",
    "    text,\n",
    "    add_special_tokens=True,      # GPT-2 will add its own BOS/EOS style if configured\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"GPT-2 token strings:\\n\", tok_gpt2.convert_ids_to_tokens(enc_gpt2[\"input_ids\"][0]))\n",
    "print(\"\\nGPT-2 token IDs:\\n\", enc_gpt2[\"input_ids\"][0].tolist())\n",
    "print(\"\\nGPT-2 attention mask:\\n\", enc_gpt2[\"attention_mask\"][0].tolist())\n",
    "\n",
    "decoded_gpt2 = tok_gpt2.decode(enc_gpt2[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(\"\\nGPT-2 decoded:\\n\", decoded_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByeJXpiEE9DS"
   },
   "source": [
    "### Quick observations\n",
    "\n",
    "- **BERT (WordPiece):** You’ll see `[CLS]` at start and `[SEP]` at end; uncommon words break into `##subwords`.\n",
    "- **GPT-2 (BPE):** No `[CLS]/[SEP]`, different subword splits; by default no PAD token.\n",
    "- **Attention mask:** All 1s here (no real padding needed for a single sentence), but essential when batching sequences of different lengths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX7fWv3uFw1Y"
   },
   "source": [
    "##D. Hugging Face `pipeline`\n",
    "\n",
    "The Hugging Face `pipeline` is a high-level API that makes it easy to use pre-trained models for common tasks without worrying about low-level details.  \n",
    "\n",
    "Some example tasks include:  \n",
    "- **Sentiment Analysis** → Detect positive/negative tone  \n",
    "- **Text Generation** → Generate new text based on a prompt  \n",
    "- **Translation** → Translate text between languages  \n",
    "- **Question Answering** → Extract answers from a passage  \n",
    "\n",
    "We’ll start with a **sentiment analysis** pipeline as our first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment-analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test on a sample sentence\n",
    "result = sentiment_analyzer(\"I am excited to learn about Hugging Face and LLMs!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsXIXINCF8tt"
   },
   "source": [
    "What happens when we run a pipeline for the first time?\n",
    "\n",
    "- When we call `pipeline(\"sentiment-analysis\")`, Hugging Face automatically:  \n",
    "  1. **Downloads** the pre-trained model (if not already cached).  \n",
    "  2. **Loads** the model into memory using PyTorch (since we installed `torch`).  \n",
    "  3. **Runs inference** on the input text and gives structured output.  \n",
    "\n",
    "- The model will be stored in a local cache (usually under `~/.cache/huggingface/transformers/`),  \n",
    "  so the next time you run it, it won’t need to download again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d24tZKYwGKmJ"
   },
   "source": [
    "### Running Sentiment Analysis on Multiple Sentences\n",
    "\n",
    "We can pass a list of sentences to the `pipeline` instead of a single string.  \n",
    "This allows us to analyze multiple inputs in one go.  \n",
    "\n",
    "This is useful when you want to quickly check several text samples without looping manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I love learning new things about AI!\",\n",
    "    \"This tutorial is a bit challenging, but I’m getting there.\",\n",
    "    \"I really don’t enjoy debugging errors late at night...\"\n",
    "]\n",
    "\n",
    "results = sentiment_analyzer(texts)\n",
    "\n",
    "for text, result in zip(texts, results): #zip() function takes multiple iterables (like lists, tuples, or strings) as arguments and returns an iterator of tuples. Each tuple contains corresponding elements from the input iterables.\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {result['label']}, Score: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHlL6FkmGurA"
   },
   "source": [
    "### Using a Specific Pretrained Model\n",
    "\n",
    "By default, the pipeline uses a standard pretrained model chosen by Hugging Face.  \n",
    "However, Hugging Face hosts thousands of models, and we can specify which one to use by passing the model name.  \n",
    "\n",
    "For example, we'll use **`distilbert-base-uncased-finetuned-sst-2-english`**,  \n",
    "a lightweight model trained specifically for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "result = custom_classifier(\"Hugging Face makes NLP super easy!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe-fRZ2EG4OW"
   },
   "source": [
    "Why Use Custom Models?\n",
    "\n",
    "- **Flexibility**: Different tasks may need different models. Hugging Face provides thousands of them.  \n",
    "- **Performance Trade-off**:  \n",
    "  - Some models are smaller (faster but slightly less accurate).  \n",
    "  - Others are larger (more accurate but slower).  \n",
    "- **Domain Specificity**: You might need models fine-tuned for finance, healthcare, legal text, or even social media.  \n",
    "- **Control**: Choosing a model explicitly ensures reproducibility instead of depending on Hugging Face's defaults (which might change).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWf_xKnGHE5P"
   },
   "source": [
    "### Batch Processing with Pipelines\n",
    "\n",
    "- The Hugging Face pipeline can take a **list of inputs** instead of just one string.  \n",
    "- This is useful when analyzing many texts together, saving time and improving efficiency.  \n",
    "- Each input is processed independently, and the results are returned in the same order as the inputs.  \n",
    "\n",
    "In practice:  \n",
    "- For **real-world applications**, you’ll often deal with a dataset or a collection of sentences.  \n",
    "- Batch processing allows you to send them together instead of calling the model one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the sentiment analysis model on multiple inputs (batch processing)\n",
    "\n",
    "results = custom_classifier([\n",
    "    \"I love how simple Hugging Face makes NLP!\",\n",
    "    \"This is too complicated, I don't like it.\",\n",
    "    \"The course is okay, but could be better.\"\n",
    "])\n",
    "\n",
    "for idx, res in enumerate(results, 1):\n",
    "    print(f\"Sentence {idx}: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1Gi48JBHRYp"
   },
   "source": [
    "### Controlling Model Outputs with `return_all_scores`\n",
    "\n",
    "- By default, the pipeline only returns the **most likely label** with its score.  \n",
    "- If we set `return_all_scores=True`, the pipeline will return the **probability for every possible label**.  \n",
    "- This gives us a more detailed view of how confident the model is across different classes.  \n",
    "\n",
    "For example:  \n",
    "- Instead of just saying `\"LABEL_1: Positive\"`, we also see the probabilities for `\"Negative\"` and `\"Neutral\"`.  \n",
    "- This is useful for applications where you care about **model confidence** and not just the top prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting detailed scores for all possible labels\n",
    "\n",
    "detailed_results = custom_classifier(\n",
    "    \"I had an average experience, nothing too exciting.\",\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "print(detailed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpky9ux9Hl_f"
   },
   "source": [
    "Getting Probabilities for All Labels (`top_k=None`)\n",
    "\n",
    "- Earlier, we used `return_all_scores=True`, but this is now deprecated.  \n",
    "- The new way is to set `top_k=None`.  \n",
    "- This returns the probability scores for **all possible labels** instead of just the top prediction.  \n",
    "- Useful when we want to analyze model confidence across all categories, not just the winner.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting detailed scores for all possible labels using top_k=None\n",
    "detailed_results = custom_classifier(\n",
    "    \"I had an average experience, nothing too exciting.\",\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "print(detailed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARb1i2U8H7_n"
   },
   "source": [
    "### Formatting Sentiment Analysis Results\n",
    "\n",
    "- The model output contains both **label** (Positive/Negative/Neutral) and **score** (confidence level).  \n",
    "- By formatting the results, we can make them easier to read.  \n",
    "- This step is useful when presenting results to end-users or when logging model predictions in a real application.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting and formatting results for better readability\n",
    "texts = [\n",
    "    \"I love Hugging Face, it makes NLP fun!\",\n",
    "    \"This is too confusing and frustrating.\",\n",
    "    \"The movie was decent, not great but not bad.\"\n",
    "]\n",
    "\n",
    "results = custom_classifier(texts)\n",
    "\n",
    "for text, res in zip(texts, results):\n",
    "    label = res['label']\n",
    "    score = round(res['score'], 4)\n",
    "    print(f\"Text: {text}\\n → Sentiment: {label} (Confidence: {score})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UA8dGeW_ILvG"
   },
   "source": [
    "### Zero-Shot Classification\n",
    "\n",
    "- Unlike sentiment analysis, **Zero-Shot Classification** allows us to classify text into **any set of categories** we choose, without needing a custom-trained model.  \n",
    "- This works using **Natural Language Inference (NLI)**:  \n",
    "  - The model checks if the text *entails* or *contradicts* a given label.  \n",
    "- In our example, we provide the labels: `education`, `politics`, `sports`, `technology`.  \n",
    "- The model then assigns probabilities to each label, showing how well the text fits each category.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zero-shot classification pipeline\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Example text\n",
    "text = \"I have been reading a lot about deep learning and transformers lately.\"\n",
    "\n",
    "# Candidate labels (categories we want to test against)\n",
    "candidate_labels = [\"education\", \"politics\", \"sports\", \"technology\"]\n",
    "\n",
    "# Perform classification\n",
    "result = zero_shot_classifier(text, candidate_labels)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEV3MR3dJKTG"
   },
   "source": [
    "Exploring Zero-Shot Classification Outputs in Detail\n",
    "\n",
    "When we use the `zero-shot-classification` pipeline, Hugging Face returns a dictionary containing:\n",
    "- **`labels`** → the candidate labels we provided (in descending order of probability).\n",
    "- **`scores`** → the probability assigned to each label.\n",
    "- **`sequence`** → the original input sentence.\n",
    "\n",
    "This means the model doesn't just give one label, but instead ranks all the labels we asked it to compare against.\n",
    "\n",
    "By inspecting the output, we can:\n",
    "- See which labels the model thinks are most relevant.\n",
    "- Understand the relative confidence (scores) for each label.\n",
    "- Verify whether the label order matches our expectations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper look into zero-shot classification outputs\n",
    "detailed_result = zero_shot_classifier(\n",
    "    \"I enjoy solving challenging machine learning problems.\",\n",
    "    candidate_labels=[\"education\", \"sports\", \"technology\", \"entertainment\", \"work\"],\n",
    ")\n",
    "\n",
    "print(detailed_result)  # Raw output\n",
    "\n",
    "# Pretty printing\n",
    "for label, score in zip(detailed_result['labels'], detailed_result['scores']):\n",
    "    print(f\"{label:<15} --> {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract labels and scores\n",
    "labels = detailed_result[\"labels\"]\n",
    "scores = detailed_result[\"scores\"]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(labels, scores, color=\"skyblue\")\n",
    "plt.xlabel(\"Confidence Score\")\n",
    "plt.title(\"Zero-Shot Classification Results\")\n",
    "plt.gca().invert_yaxis()  # Highest score on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0X7YklpJ_hO"
   },
   "source": [
    "### Conclusion on Hugging Face Pipelines\n",
    "\n",
    "We have explored Hugging Face pipelines for **sentiment analysis** and **zero-shot classification**.  \n",
    "Similarly, pipelines can be used for a wide range of NLP tasks such as:\n",
    "\n",
    "- **Question Answering**  \n",
    "- **Named Entity Recognition (NER)**  \n",
    "- **Text Summarization**  \n",
    "- **Translation**  \n",
    "- **Text Generation**  \n",
    "\n",
    "The key point is that **pipelines abstract away the complexities** of how transformers work internally.  \n",
    "Under the hood, every pipeline is performing the following steps:\n",
    "\n",
    "1. **Tokenization** – Converting input text into tokens and token IDs.  \n",
    "2. **Model Forward Pass** – Feeding token IDs into a pre-trained model.  \n",
    "3. **Post-Processing** – Converting model outputs into human-readable results.  \n",
    "\n",
    "This makes pipelines extremely convenient for quick experimentation, but to truly understand how transformers process text, we need to go one level deeper.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LcgYwTnKKhE"
   },
   "source": [
    "## E. From Tokens to Model Inputs\n",
    "\n",
    "When we call a tokenizer, we don’t just want raw tokens.  \n",
    "For models, the tokenizer prepares a full set of inputs:\n",
    "\n",
    "- **`input_ids`** → numerical IDs for each token  \n",
    "- **`attention_mask`** → tells the model which tokens are real and which are padding  \n",
    "- **`token_type_ids`** (optional, used in models like BERT for sentence pairs)\n",
    "\n",
    "Let’s tokenize our sentence into model-ready inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Prepare model-ready inputs\n",
    "inputs = tokenizer(\n",
    "    \"Hugging Face makes NLP simple and powerful!\",\n",
    "    return_tensors=\"pt\"   # return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA4v3q0OKvNt"
   },
   "source": [
    "### Passing Tokenized Inputs to the Model\n",
    "\n",
    "Now that we have tokenized inputs, we can pass them into a pretrained model.  \n",
    "For this, we’ll use **BERT base uncased** (`bert-base-uncased`).  \n",
    "\n",
    "The model outputs:\n",
    "- **`last_hidden_state`** → embeddings for each token in the input  \n",
    "- **`pooler_output`** → a single embedding for the whole sentence (commonly the `[CLS]` token)\n",
    "\n",
    "These are the raw hidden representations (embeddings) before any task-specific head like classification is applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If using CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_n5RbynLpZ6"
   },
   "source": [
    "Why Set a Manual Seed?\n",
    "\n",
    "When working with deep learning models, some operations can be non-deterministic  \n",
    "(e.g., weight initialization, dropout, CUDA kernels).  \n",
    "By setting a **manual seed**, we make sure that:\n",
    "\n",
    "- Random number generation is consistent across runs  \n",
    "- Embeddings and results (when randomness is involved) remain reproducible  \n",
    "- Debugging and comparing outputs becomes easier\n",
    "\n",
    "In PyTorch, we use `torch.manual_seed()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Load BERT model (without any classification head)\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Pass inputs through the model\n",
    "with torch.no_grad():  # no gradient calculation, just inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract embeddings\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "\n",
    "print(\"Last hidden state shape:\", last_hidden_state.shape)\n",
    "print(\"Pooler output shape:\", pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b1Hopz_LKqz"
   },
   "source": [
    "Visualizing Token Embeddings\n",
    "\n",
    "Each token in the input sentence is mapped to a high-dimensional vector (embedding).  \n",
    "The `last_hidden_state` gives us these embeddings:\n",
    "\n",
    "- Shape: `[batch_size, sequence_length, hidden_size]`\n",
    "- For **BERT base**, `hidden_size = 768`\n",
    "\n",
    "We can inspect:\n",
    "1. The tokens\n",
    "2. Their corresponding embedding vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens back from tokenizer\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Pick embeddings for the first sentence in the batch\n",
    "embeddings = last_hidden_state[0]  # shape: (seq_len, hidden_size)\n",
    "\n",
    "# Print tokens with first 5 dimensions of their embeddings\n",
    "for token, vector in zip(tokens, embeddings):\n",
    "    print(f\"{token:10s} -> {vector[:5].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BjUlCd5QqKf"
   },
   "source": [
    "Sentence Embeddings with `pooler_output`\n",
    "\n",
    "So far, we’ve looked at **token embeddings** (one vector per token).  \n",
    "But many tasks require a **single vector for the entire sentence** (e.g., similarity, clustering, classification).  \n",
    "\n",
    "- In BERT-based models, the **`pooler_output`** provides a fixed-size embedding for the whole input.  \n",
    "- It’s derived from the `[CLS]` token representation, passed through a linear layer + `tanh` activation.  \n",
    "- This makes it suitable for downstream tasks like classification and semantic similarity.  \n",
    "\n",
    "We’ll extract `pooler_output` for a sentence to get its embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Hugging Face makes working with transformers easy!\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Get outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract sentence embedding\n",
    "sentence_embedding = outputs.pooler_output  # shape: [1, hidden_size]\n",
    "\n",
    "print(\"Sentence Embedding Shape:\", sentence_embedding.shape)\n",
    "print(\"Sentence Embedding Vector (first 5 values):\", sentence_embedding[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0BLBBUx9Zn0"
   },
   "source": [
    "Comparing `pooler_output` vs Mean Pooling for Sentence Embeddings\n",
    "\n",
    "BERT provides two common ways to obtain sentence-level embeddings:\n",
    "\n",
    "1. **`pooler_output`**  \n",
    "   - Based only on the `[CLS]` token representation.  \n",
    "   - Passed through a linear layer and `tanh` activation.  \n",
    "   - Often used for classification tasks in BERT’s original design.\n",
    "\n",
    "2. **Mean Pooling**  \n",
    "   - Averages all token embeddings (excluding padding).  \n",
    "   - Often preferred in sentence-transformers and modern retrieval systems.  \n",
    "   - Captures more holistic sentence information.\n",
    "\n",
    "Let’s compute both for the same sentence and compare their shapes and vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Transformers provide an easy way to work with NLP models.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 1. Pooler Output (CLS token)\n",
    "cls_embedding = outputs.pooler_output  # shape: [1, hidden_size]\n",
    "\n",
    "# 2. Mean Pooling (excluding padding)\n",
    "last_hidden = outputs.last_hidden_state  # shape: [1, seq_len, hidden_size]\n",
    "attention_mask = inputs['attention_mask']\n",
    "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "sum_embeddings = torch.sum(last_hidden * mask_expanded, 1)\n",
    "sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "mean_embedding = sum_embeddings / sum_mask\n",
    "\n",
    "print(\"CLS Pooler Output Shape:\", cls_embedding.shape)\n",
    "print(\"Mean Pooling Shape:\", mean_embedding.shape)\n",
    "print(\"\\nFirst 5 values of CLS Pooler Output:\", cls_embedding[0][:5])\n",
    "print(\"\\nFirst 5 values of Mean Pooling:\", mean_embedding[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"Transformers are powerful for NLP tasks.\",\n",
    "    \"Natural language processing is evolving rapidly.\",\n",
    "    \"BERT is a transformer-based model.\",\n",
    "    \"I love eating pizza on weekends.\",\n",
    "    \"The weather today is sunny and pleasant.\"\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "cls_embeddings = []\n",
    "mean_embeddings = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # CLS pooler output\n",
    "    cls_embeddings.append(outputs.pooler_output.squeeze().numpy())\n",
    "\n",
    "    # Mean pooling\n",
    "    last_hidden = outputs.last_hidden_state\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "    sum_embeddings = torch.sum(last_hidden * mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "    mean_embeddings.append((sum_embeddings / sum_mask).squeeze().numpy())\n",
    "\n",
    "# Convert to numpy\n",
    "import numpy as np\n",
    "cls_embeddings = np.array(cls_embeddings)\n",
    "mean_embeddings = np.array(mean_embeddings)\n",
    "\n",
    "# PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "cls_2d = pca.fit_transform(cls_embeddings)\n",
    "mean_2d = pca.fit_transform(mean_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(cls_2d[:, 0], cls_2d[:, 1], color='blue', label='CLS Pooler')\n",
    "plt.scatter(mean_2d[:, 0], mean_2d[:, 1], color='red', label='Mean Pooling')\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    plt.annotate(f\"S{i+1}\", (cls_2d[i, 0], cls_2d[i, 1]), color='blue')\n",
    "    plt.annotate(f\"S{i+1}\", (mean_2d[i, 0], mean_2d[i, 1]), color='red')\n",
    "\n",
    "plt.title(\"Sentence Embeddings: CLS Pooler vs Mean Pooling (PCA 2D)\")\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimension 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZaQiq7h925D"
   },
   "source": [
    "Visualization of Sentence Embeddings: CLS Pooler vs Mean Pooling\n",
    "\n",
    "* Blue points = CLS pooler embeddings\n",
    "* Red points = Mean pooling embeddings\n",
    "* Labels S1–S5 correspond to sentences\n",
    "\n",
    "The graph above shows a **2D representation of sentence embeddings** obtained using two different methods from the BERT model:\n",
    "\n",
    "- **CLS Pooler Output (Blue points):** Uses the final hidden state of the special `[CLS]` token, which is trained during BERT’s pretraining for classification-like tasks.  \n",
    "- **Mean Pooling (Red points):** Computes the mean of all token embeddings (considering the attention mask), providing a more generalized representation of the sentence.\n",
    "\n",
    "Each point (S1–S5) represents one sentence from the input set. Sentences that are semantically closer tend to appear nearer in the 2D space, though this is an approximation.\n",
    "\n",
    "---\n",
    "\n",
    "Why PCA?\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to project high-dimensional data (here, 768-dimensional BERT embeddings) into a lower-dimensional space (2D for visualization).\n",
    "\n",
    "Key points about PCA:\n",
    "- It identifies the directions (principal components) in which the data varies the most.\n",
    "- The first component explains the highest variance, the second explains the next highest, and so on.\n",
    "- By using only the first two components, we can plot the embeddings in a way that preserves as much of their variance (structure) as possible while making it human-interpretable.\n",
    "\n",
    "---\n",
    "\n",
    "This visualization allows us to see how different pooling strategies (CLS vs Mean) can create slightly different spatial relationships between sentence embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK27hDLi-0ia"
   },
   "source": [
    "Semantic Similarity Comparison: CLS Pooler vs Mean Pooling\n",
    "\n",
    "We computed the **cosine similarity** between sentence embeddings generated using two different methods:\n",
    "\n",
    "- **CLS Pooler Output:** Uses the `[CLS]` token representation.\n",
    "- **Mean Pooling:** Averages all token embeddings for the sentence.\n",
    "\n",
    "---\n",
    "\n",
    "Why Cosine Similarity?\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors in high-dimensional space.  \n",
    "It ranges from **-1 (completely opposite)** to **+1 (identical)**.  \n",
    "For sentence embeddings:\n",
    "- **Higher cosine similarity** → more semantically similar.\n",
    "- **Lower cosine similarity** → less semantically related.\n",
    "\n",
    "---\n",
    "\n",
    "Observations\n",
    "\n",
    "- **CLS Pooler:** Sometimes fails to capture nuanced relationships, as it was originally trained for classification tasks (e.g., NSP) rather than sentence-level similarity.\n",
    "- **Mean Pooling:** Often produces more stable and semantically meaningful similarities across diverse sentences, especially when fine-tuning is not performed.\n",
    "\n",
    "---\n",
    "\n",
    "When to Use Which?\n",
    "\n",
    "- Use **Mean Pooling** when building general-purpose sentence embeddings (e.g., clustering, semantic search, retrieval).\n",
    "- Use **CLS Pooler** when fine-tuning the model for a specific task where `[CLS]` is trained to represent sentence meaning.\n",
    "\n",
    "---\n",
    "\n",
    "This step helps us decide which embedding strategy is better suited before proceeding to **training or fine-tuning for downstream NLP tasks**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Manual seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"The weather is sunny today.\",\n",
    "    \"I enjoy coding in Python.\",\n",
    "    \"Artificial intelligence is fascinating.\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state   # (batch, seq_len, hidden_dim)\n",
    "    cls_pooler_output = outputs.pooler_output        # (batch, hidden_dim)\n",
    "\n",
    "# Mean pooling: average all token embeddings (excluding padding)\n",
    "attention_mask = inputs['attention_mask'].unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "masked_hidden = last_hidden_states * attention_mask\n",
    "mean_pooling = masked_hidden.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "\n",
    "# Compute cosine similarity matrices\n",
    "cls_similarity = cosine_similarity(cls_pooler_output)\n",
    "mean_similarity = cosine_similarity(mean_pooling)\n",
    "\n",
    "# Plot heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(cls_similarity, annot=True, xticklabels=sentences, yticklabels=sentences, cmap=\"coolwarm\", ax=axes[0])\n",
    "axes[0].set_title(\"Cosine Similarity - CLS Pooler\")\n",
    "\n",
    "sns.heatmap(mean_similarity, annot=True, xticklabels=sentences, yticklabels=sentences, cmap=\"coolwarm\", ax=axes[1])\n",
    "axes[1].set_title(\"Cosine Similarity - Mean Pooling\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iirssrirAynw"
   },
   "source": [
    "## Clustering Sentence Embeddings with KMeans\n",
    "\n",
    "After obtaining sentence embeddings using **mean pooling**, we applied **KMeans clustering** to group similar sentences.\n",
    "\n",
    "### Why KMeans?\n",
    "\n",
    "KMeans partitions data into *k* clusters by minimizing the within-cluster variance.  \n",
    "It is commonly used for:\n",
    "- Grouping semantically similar texts\n",
    "- Organizing large text corpora\n",
    "- Topic discovery in unsupervised NLP\n",
    "\n",
    "### Steps Performed\n",
    "\n",
    "1. **Embedding Selection:** Used mean-pooled BERT embeddings.\n",
    "2. **KMeans Clustering:** Chose `k=2` for our small dataset (can be tuned).\n",
    "3. **Dimensionality Reduction:** Applied PCA to project 768-dimensional embeddings into 2D for visualization.\n",
    "4. **Cluster Interpretation:** Each cluster groups sentences with related meanings.\n",
    "\n",
    "### Observations\n",
    "\n",
    "- Sentences about machine learning and AI formed one cluster.\n",
    "- Unrelated sentences (e.g., weather, general coding) tended to form another cluster.\n",
    "\n",
    "This sets the foundation for tasks like **semantic search**, **document organization**, and **topic modeling**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QKqzQgySQmQ"
   },
   "source": [
    "# **Day 2 Summary: Tokenization & Embeddings**\n",
    "\n",
    "This summary documents the theoretical and conceptual learning from **Day 2**, covering tokenization and embeddings in Hugging Face.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Tokenization**\n",
    "\n",
    "### **What is Tokenization?**\n",
    "\n",
    "* Tokenization is the process of splitting raw text into **tokens** (subwords, words, or characters) that a model can process.\n",
    "* BERT uses **WordPiece tokenization**, while models like GPT-2 use **Byte-Pair Encoding (BPE)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How is it Performed?**\n",
    "\n",
    "* Hugging Face provides **`AutoTokenizer`** to automatically load the correct tokenizer:\n",
    "\n",
    "  * `from transformers import AutoTokenizer`\n",
    "  * `tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "* Key functions:\n",
    "\n",
    "  * **`tokenizer()`** – Encodes text into input IDs, attention masks, etc.\n",
    "  * Parameters:\n",
    "\n",
    "    * `padding='max_length'` – Pads to a fixed length.\n",
    "    * `truncation=True` – Truncates longer sequences.\n",
    "    * `return_tensors='pt'` – Returns tensors for PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### **Where is it Used?**\n",
    "\n",
    "* **Preprocessing step** in all NLP tasks: classification, QA, summarization, generation, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### **When is it Applied?**\n",
    "\n",
    "* **Before inference or training**, to convert text into model-readable format.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Tokenization Important?**\n",
    "\n",
    "* Models work on **token IDs, not raw text**.\n",
    "* Subword tokenization handles **rare, unknown, or misspelled words** effectively while keeping vocabulary size manageable.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Embeddings**\n",
    "\n",
    "### **What Are Embeddings?**\n",
    "\n",
    "* **Dense numerical vectors** representing tokens, words, or entire sentences.\n",
    "* Capture **semantic meaning and context**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Are They Generated?**\n",
    "\n",
    "* Use Hugging Face **`AutoModel`** to get hidden states:\n",
    "\n",
    "  * `from transformers import AutoModel`\n",
    "  * `model = AutoModel.from_pretrained(\"bert-base-uncased\")`\n",
    "* Output:\n",
    "\n",
    "  * **`last_hidden_state`** – Embeddings for each token.\n",
    "  * **`pooler_output`** – Embedding representing the entire sentence (via `[CLS]` token in BERT).\n",
    "\n",
    "---\n",
    "\n",
    "### **Where Are They Used?**\n",
    "\n",
    "* In **downstream tasks** like:\n",
    "\n",
    "  * Text classification\n",
    "  * Semantic search\n",
    "  * Clustering\n",
    "  * Question answering\n",
    "  * Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use CLS vs Mean Pooling?**\n",
    "\n",
    "* **CLS (`pooler_output`)**: Best for classification tasks.\n",
    "* **Mean pooling (averaging token embeddings)**: Best for sentence similarity, clustering, or retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Are Embeddings Important?**\n",
    "\n",
    "* Bridge between **human language** and **machine learning algorithms**.\n",
    "* Enable computation of **similarity (cosine, Euclidean)** and facilitate **transfer learning**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Hyperparameters & Parameters**\n",
    "\n",
    "* **`max_length`**: Controls sequence length during tokenization.\n",
    "* **`return_tensors`**: Decides the output tensor type (`'pt'` for PyTorch, `'tf'` for TensorFlow).\n",
    "* **Pooling method**: `mean`, `max`, or `cls`.\n",
    "* **Normalization**: Optional step (e.g., L2 normalization) for similarity-based tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
