{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qUodetLM9wk"
   },
   "source": [
    "## What is Retrieval-Augmented Generation (RAG)?\n",
    "\n",
    "RAG is a technique that enhances Large Language Models (LLMs) by combining:\n",
    "1. **Retrieval** – Search a knowledge base for the most relevant documents.\n",
    "2. **Generation** – Feed those retrieved documents into an LLM to produce a context-aware answer.\n",
    "\n",
    "### Why RAG?\n",
    "- LLMs have a fixed knowledge cutoff (they \"forget\" recent or external info).\n",
    "- Retrieval allows dynamic access to updated or domain-specific knowledge.\n",
    "- It reduces hallucinations by grounding responses in real data.\n",
    "\n",
    "**Example:**\n",
    "- Question: *\"What is LoRA fine-tuning?\"*\n",
    "- RAG retrieves your Day 5 summary about LoRA → LLM generates a concise explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV8fvK4ZNpC6"
   },
   "source": [
    "### How Does LoRA Complement RAG?\n",
    "\n",
    "While RAG (Retrieval-Augmented Generation) focuses on **grounding a language model's answers using external knowledge**, LoRA (Low-Rank Adaptation) enhances **how the model expresses that knowledge**.\n",
    "\n",
    "### Why Combine Them?\n",
    "- **RAG's Role**: Fetches and injects relevant, up-to-date information from a database or vector store.\n",
    "- **LoRA's Role**: Fine-tunes only a small subset of the model's parameters to adapt its tone, style, or domain-specific understanding.\n",
    "\n",
    "### Benefits of RAG + LoRA\n",
    "1. **Domain-Specific RAG Pipelines**: Fine-tune the LLM via LoRA to speak in your company's language while RAG fetches fresh policies, research, or FAQs.\n",
    "2. **Lightweight & Efficient**: LoRA fine-tuning affects only ~1–2% of the parameters, making it cost-effective for frequently updated domains.\n",
    "3. **Reduced Hallucinations**: RAG ensures the facts are correct, and LoRA ensures they are presented in a tailored, meaningful way.\n",
    "\n",
    "**Example:**\n",
    "- Build a RAG-based assistant for healthcare.\n",
    "- Use LoRA fine-tuning to teach the LLM medical terminology and patient-friendly explanation styles.\n",
    "- Result: Answers are both *accurate (via RAG)* and *professionally aligned (via LoRA)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA1EL9-jO6ZL"
   },
   "source": [
    "## Setting Up the Retriever\n",
    "\n",
    "A retriever is responsible for fetching the **top-k most relevant documents** from a knowledge base, given a user query.\n",
    "\n",
    "In our case:\n",
    "- Knowledge base = Wikipedia summaries on AI/ML/Data Science\n",
    "- Representation = Sentence embeddings (vector form of summaries).\n",
    "- Similarity metric = Cosine similarity.\n",
    "\n",
    "We'll recreate the knowledge base:\n",
    "1. Define a list of topics related to AI, ML, and Data Science.\n",
    "2. Fetch their summaries using Python's `wikipedia` library.\n",
    "3. Store the results as a JSON file inside an `artifacts/` folder.\n",
    "4. Generate sentence embeddings for each summary using a Transformer model.\n",
    "\n",
    "This knowledge base will serve as the retrieval source for our RAG pipeline.\n",
    "\n",
    "Workflow:\n",
    "1. Convert user query into an embedding (same model used for corpus).\n",
    "2. Compute cosine similarity between query embedding and all stored embeddings.\n",
    "3. Select top-k most similar documents to feed into the generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9YG03q2SCQi",
    "outputId": "7c3d824b-b0c1-4b7f-d23b-77abdf3f6341"
   },
   "outputs": [],
   "source": [
    "!pip install wikipedia\n",
    "topics = [\n",
    "    'Artificial Intelligence',\n",
    "    'Machine Learning',\n",
    "    'Deep Learning',\n",
    "    'Neural Networks',\n",
    "    'Generative AI',\n",
    "    'Computer Vision',\n",
    "    'Large Language Model',\n",
    "    'Retrieval-augmented generation',\n",
    "    'Object Detection',\n",
    "    'Face Recognition',\n",
    "    'Natural Language Processing',\n",
    "    'Image Processing',\n",
    "    'Data Science',\n",
    "    'Data Mining',\n",
    "    'Big Data',\n",
    "    'Data Analytics',\n",
    "    'Predictive Analytics',\n",
    "    'Statistical Modeling',\n",
    "    'Data Visualization',\n",
    "    'Exploratory Data Analysis',\n",
    "    'Data Cleaning',\n",
    "    'ETL (Extract Transform Load)',\n",
    "    'Business Intelligence',\n",
    "    'Data Warehousing',\n",
    "    'Feature Engineering',\n",
    "    'Time Series Analysis',\n",
    "    'Reinforcement Learning',\n",
    "    'Anomaly Detection',\n",
    "    'Data Governance',\n",
    "    'Data Ethics',\n",
    "    'Cloud Computing for Data Science'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U53_yF8IMP8A",
    "outputId": "acd3788e-0dea-4f92-f8e8-b31227ec5f03"
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create artifacts folder if not exists\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "successful_topics = []  # track which topics succeeded\n",
    "corpus = []             # your summaries\n",
    "\n",
    "for topic in topics:\n",
    "    try:\n",
    "        search_results = wikipedia.search(topic)\n",
    "        if not search_results:\n",
    "            print(f\"No results for: {topic}\")\n",
    "            continue\n",
    "        page = wikipedia.page(search_results[0])\n",
    "        corpus.append({\n",
    "            \"requested_topic\": topic,\n",
    "            \"fetched_title\": page.title,\n",
    "            \"summary\": page.summary\n",
    "        })\n",
    "        successful_topics.append(page.title)  # store actual page title\n",
    "        print(f\"Fetched: {page.title}\")\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Skipped {topic} due to disambiguation: {e}\")\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Page not found for: {topic}\")\n",
    "\n",
    "print(f\"\\nTotal successful topics: {len(successful_topics)}\")\n",
    "\n",
    "# Save corpus\n",
    "with open(\"artifacts/wikipedia_corpus_rag.json\", \"w\") as f:\n",
    "    json.dump(corpus, f, indent=2)\n",
    "\n",
    "print(f\"\\nTotal valid articles fetched: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xKcTyvJTG1y"
   },
   "source": [
    "## Generate Embeddings for Knowledge Base\n",
    "\n",
    "To enable fast and meaningful retrieval in RAG, we convert each document's summary\n",
    "into a dense numerical representation (embedding). These embeddings capture\n",
    "semantic meaning, allowing similar concepts to be close in vector space.\n",
    "\n",
    "- We use a pre-trained sentence embedding model from Hugging Face\n",
    "  (e.g., `sentence-transformers/all-MiniLM-L6-v2`).\n",
    "- Each summary is transformed into a 384-dimensional vector.\n",
    "- These vectors will be stored alongside their corresponding topic titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "df15e78d05494c87bb521a8722004762",
      "0202aec5376d478b9f930bfa3d296376",
      "1f4d29359efa4bc1bb9f61ef7447fd4f",
      "46e3cced7ddc490abb298a706ddcb5ca",
      "346b95f14515407a948e5b8a94c16eb3",
      "f2aae9e683a94ed5aa1bebe408aea3d6",
      "59a0013f3398480882fcdd3ed5f646cc",
      "4a62e69d518f4df58e2f85d07b4d0cd3",
      "e5a13e4593814d909c2f657cc47db74f",
      "51deec0a7878438f81451d960d643d22",
      "c135e39f87954861aa97dff48274ca7b"
     ]
    },
    "id": "yW4WzdN6SjXn",
    "outputId": "a8642cb5-70ac-46cd-e09e-a6273ecafa07"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load the previously saved corpus\n",
    "with open(\"artifacts/wikipedia_corpus_rag.json\", \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Extract summaries\n",
    "texts = [item[\"summary\"] if isinstance(item, dict) else item for item in corpus]\n",
    "\n",
    "# Load a sentence embedding model (lightweight, free-tier friendly)\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Save embeddings\n",
    "np.save(\"artifacts/wikipedia_embeddings_rag.npy\", embeddings)\n",
    "\n",
    "print(f\"Generated embeddings for {len(texts)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5fLbVgCUwar"
   },
   "source": [
    "## Visualize Embeddings using PCA\n",
    "\n",
    "To confirm the quality of our generated embeddings, we reduce their dimensionality from 384 → 2\n",
    "using **Principal Component Analysis (PCA)**. This allows us to:\n",
    "- Visually inspect how different topics are distributed in semantic space.\n",
    "- Check for any obvious clustering or overlap.\n",
    "\n",
    "Interpretation:\n",
    "- Each point represents one Wikipedia topic summary.\n",
    "- Closer points → more semantically similar content.\n",
    "- Farther points → less related content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "GyQc7XyzUchq",
    "outputId": "3425f18f-1eeb-450a-98af-1098ef4afb19"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load corpus and embeddings\n",
    "with open(\"artifacts/wikipedia_corpus_rag.json\", \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "embeddings = np.load(\"artifacts/wikipedia_embeddings_rag.npy\")\n",
    "\n",
    "# Extract topic names (handles dict or simple list)\n",
    "topics = [item[\"title\"] if isinstance(item, dict) and \"title\" in item else\n",
    "          item.get(\"topic\", f\"Topic {idx}\") if isinstance(item, dict)\n",
    "          else f\"Topic {idx}\" for idx, item in enumerate(corpus)]\n",
    "\n",
    "# Reduce dimensions using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c='blue', alpha=0.6)\n",
    "\n",
    "# Annotate each point\n",
    "for i, topic in enumerate(topics):\n",
    "    plt.annotate(topic[:20] + \"...\", (embeddings_2d[i, 0] + 0.02, embeddings_2d[i, 1]))\n",
    "\n",
    "plt.title(\"Wikipedia RAG Corpus – PCA Visualization\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH4tlV2RVbbI"
   },
   "source": [
    "## Build a Retriever Function\n",
    "\n",
    "The retriever is the backbone of a RAG system. Its job:\n",
    "- Take a **query** as input.\n",
    "- Convert it into an embedding (using the same model as the corpus).\n",
    "- Compute **cosine similarity** between the query embedding and all document embeddings.\n",
    "- Return the **top-k most relevant documents**.\n",
    "\n",
    "Why cosine similarity?\n",
    "- Measures how close two vectors are in terms of direction (not magnitude).\n",
    "- Works well for high-dimensional embeddings like those produced by MiniLM/BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XElz-AxjVafM",
    "outputId": "6ef02023-4ffb-4c89-8926-39b7ac0818e8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load corpus and embeddings\n",
    "with open(\"artifacts/wikipedia_corpus_rag.json\", \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "embeddings = np.load(\"artifacts/wikipedia_embeddings_rag.npy\")\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Retrieve function\n",
    "def retrieve_top_k(query, k=5):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    top_k_idx = np.argsort(similarities)[::-1][:k]\n",
    "    results = []\n",
    "    for idx in top_k_idx:\n",
    "        topic_name = corpus[idx][\"title\"] if isinstance(corpus[idx], dict) and \"title\" in corpus[idx] else f\"Topic {idx}\"\n",
    "        summary = corpus[idx][\"summary\"] if isinstance(corpus[idx], dict) and \"summary\" in corpus[idx] else corpus[idx]\n",
    "        results.append({\n",
    "            \"topic\": topic_name,\n",
    "            \"similarity\": float(similarities[idx]),\n",
    "            \"summary\": summary\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Test it\n",
    "query = \"How do neural networks learn?\"\n",
    "top_results = retrieve_top_k(query, k=5)\n",
    "for r in top_results:\n",
    "    print(f\"Topic: {r['topic']}\\nScore: {r['similarity']:.4f}\\nSummary: {r['summary'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBPJbc8QWBqJ"
   },
   "source": [
    "Suppose we have:\n",
    "\n",
    "Corpus (3 documents):\n",
    "1. \"Cats are small animals that like to sleep.\"\n",
    "2. \"Dogs are loyal pets and like to play.\"\n",
    "3. \"Neural networks are used in artificial intelligence.\"\n",
    "\n",
    "Query: \"Tell me about pets.\"\n",
    "\n",
    "1. Encode Corpus (already done once and saved) - Each document is turned into an embedding vector.\n",
    "2. Encode Query - The query \"Tell me about pets.\" → [0.33, 0.87, 0.15, ...]\n",
    "3. Calculate Cosine Similarity - Compare the query vector with each document vector:\n",
    "  - Cosine(Query, Doc1) = 0.62\n",
    "  - Cosine(Query, Doc2) = 0.91  ← highest similarity\n",
    "  - Cosine(Query, Doc3) = 0.10\n",
    "4. Sort and Pick Top-k:\n",
    "  - Sort scores in descending order: [0.91, 0.62, 0.10]\n",
    "  - Take top 2 indices → [Doc2, Doc1]\n",
    "5. Return Results - The function returns something like:\n",
    "```\n",
    "[\n",
    "    {\"topic\": \"Dogs are loyal pets...\", \"similarity\": 0.91},\n",
    "    {\"topic\": \"Cats are small animals...\", \"similarity\": 0.62}\n",
    "]\n",
    "```\n",
    "Key Points\n",
    "- query_embedding = embedder.encode([query]) → converts the search query into a vector.\n",
    "- cosine_similarity(query_embedding, embeddings) → finds how close each document is to the query.\n",
    "- np.argsort(similarities)[::-1][:k] → sorts in descending order and takes top-k.\n",
    "- Returns both the score and the matching document summary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZojaREma__6"
   },
   "source": [
    "## Integrate Retriever with a Language Model (RAG Flow)\n",
    "\n",
    "- **Goal:** Combine semantic search (retriever) with a language model (generator).\n",
    "- **Retriever:** Finds top-k most relevant documents.\n",
    "- **Generator (LLM):** Uses the retrieved context to craft a response.\n",
    "- **Benefit:** The model does not need to \"memorize\" all facts. It retrieves them dynamically.\n",
    "\n",
    "We will:\n",
    "1. Accept a query from the user.\n",
    "2. Retrieve top 2–3 summaries using cosine similarity.\n",
    "3. Concatenate these summaries into a context string.\n",
    "4. Pass this context + query to a text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Q49mIpSV17K",
    "outputId": "8e075a1d-2302-4149-c3d6-a69c2c5289ac"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a lightweight language model (can use flan-t5-base or similar)\n",
    "rag_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=-1)\n",
    "\n",
    "def rag_answer(query, top_k=3, max_context_tokens=400):\n",
    "    # Step 1: Retrieve top-k relevant summaries\n",
    "    similarities = cosine_similarity(embedder.encode([query]), embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    retrieved_docs = [texts[i] for i in top_indices]\n",
    "\n",
    "    # Step 2: Build context (truncate if too long)\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    context_tokens = context.split()\n",
    "    if len(context_tokens) > max_context_tokens:\n",
    "        context = \" \".join(context_tokens[:max_context_tokens])\n",
    "\n",
    "    # Step 3: Combine query with context\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    # Step 4: Generate answer\n",
    "    result = rag_generator(prompt, max_new_tokens=150, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "    return result[0][\"generated_text\"], [texts[i] for i in top_indices]\n",
    "\n",
    "\n",
    "# Example\n",
    "query = \"What is the role of feature engineering in data science?\"\n",
    "print(rag_answer(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3Z7-PGcd3mM"
   },
   "source": [
    "We’re hitting the token-limit warning because the prompt (context + question) is still being fed to the model with >512 tokens. Word-based truncation isn’t enough; we need token-aware truncation using the tokenizer, and we should bypass the pipeline so we control inputs precisely.\n",
    "\n",
    "Here’s a drop-in fix that:\n",
    "\n",
    "Builds context within a token budget,\n",
    "\n",
    "Tokenizes with truncation to 512,\n",
    "\n",
    "Calls the model directly (no pipeline),\n",
    "\n",
    "Eliminates the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpT-aTOIbF60",
    "outputId": "b210fb6e-d897-4192-cc06-f8453cd48116"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load generator model + tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "gen = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Helper: build context within a token budget (encoder side)\n",
    "def build_context_token_budget(retrieved_docs, query, budget=480):\n",
    "    # keep some room for instructions + question (we’ll enforce with tokenizer)\n",
    "    parts = []\n",
    "    used = len(tok.encode(f\"Question: {query}\\n\", add_special_tokens=False))\n",
    "    for doc in retrieved_docs:\n",
    "        chunk = \"\\n---\\n\" + doc\n",
    "        chunk_tokens = tok.encode(chunk, add_special_tokens=False)\n",
    "        if used + len(chunk_tokens) > budget:\n",
    "            remaining = budget - used\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            # truncate this chunk to fit the budget\n",
    "            chunk_tokens = chunk_tokens[:remaining]\n",
    "            chunk = tok.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            parts.append(chunk)\n",
    "            break\n",
    "        parts.append(chunk)\n",
    "        used += len(chunk_tokens)\n",
    "    return \"\".join(parts)\n",
    "\n",
    "# 2) RAG answer with token-aware truncation\n",
    "def rag_answer(query, top_k=3, token_budget=480, max_new_tokens=128, temperature=0.7, top_p=0.9):\n",
    "    # retrieve top-k by cosine similarity\n",
    "    sims = cosine_similarity(embedder.encode([query], convert_to_numpy=True), embeddings)[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:top_k]\n",
    "    retrieved_docs = [texts[i] for i in top_idx]\n",
    "\n",
    "    # build context within a safe token budget\n",
    "    context = build_context_token_budget(retrieved_docs, query, budget=token_budget)\n",
    "\n",
    "    # final prompt\n",
    "    prompt = (\n",
    "        \"Use the context to answer the question concisely. \"\n",
    "        \"If the context is insufficient, say you don't know.\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    # tokenize with hard truncation to model’s max input length (512 for Flan-T5)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    # generate\n",
    "    outputs = gen.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    answer = tok.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    retrieved_info = [\n",
    "        {\"rank\": r+1, \"index\": int(i), \"similarity\": float(sims[i])}\n",
    "        for r, i in enumerate(top_idx)\n",
    "    ]\n",
    "    return answer, retrieved_info\n",
    "\n",
    "# Example test\n",
    "q = \"Explain the importance of feature engineering in data science.\"\n",
    "ans, used = rag_answer(q, top_k=3)\n",
    "print(\"Answer:\\n\", ans)\n",
    "print(\"\\nRetrieved (rank, idx, sim):\\n\", used)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH5HhENifwOA"
   },
   "source": [
    "## Evaluate the RAG System's Performance\n",
    "\n",
    "Now that the RAG pipeline is working with token-safe truncation, we will:\n",
    "1. Test multiple **queries** to check the relevance of retrieved documents.\n",
    "2. Inspect the **generated answers** for correctness and grounding.\n",
    "3. View the **retrieved documents (top-k)** for each query.\n",
    "\n",
    "This helps us:\n",
    "- Understand whether truncation affects the retrieved context.\n",
    "- Verify if the model is **using retrieved knowledge** effectively.\n",
    "- Identify cases where RAG may fail or retrieve irrelevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1o07ri6eFBd",
    "outputId": "022c65c2-e114-44ff-8de3-70acb8e22a4b"
   },
   "outputs": [],
   "source": [
    "# Define a set of evaluation queries\n",
    "eval_queries = [\n",
    "    \"What is reinforcement learning?\",\n",
    "    \"How does data visualization help in analysis?\",\n",
    "    \"Explain feature engineering in machine learning.\",\n",
    "    \"What is a large language model?\",\n",
    "    \"Describe data cleansing.\"\n",
    "]\n",
    "\n",
    "# Evaluate each query\n",
    "for query in eval_queries:\n",
    "    answer, retrieved = rag_answer(query, top_k=3)\n",
    "    print(f\"\\n--- Query: {query} ---\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"Retrieved Documents (Top 3):\")\n",
    "    for r in retrieved:\n",
    "        print(f\"  Rank {r['rank']}: Index {r['index']}, Similarity {r['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOjqpXUphLFP"
   },
   "source": [
    "## Visualizing Retrieved Document Similarities\n",
    "\n",
    "To better understand how the RAG pipeline scores the documents during retrieval:\n",
    "- We will plot a **heatmap** of cosine similarity scores.\n",
    "- Each row will represent a query.\n",
    "- Each column will represent a retrieved document (Top-k).\n",
    "- Darker cells = higher similarity (more relevant context).\n",
    "\n",
    "This allows us to visually inspect:\n",
    "- Which documents the model considers most relevant for each query.\n",
    "- Whether there is consistent retrieval behavior across queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "opS8JfF0gtUg",
    "outputId": "c8a7d6a1-5cdc-4859-e0ae-4ecb5f78f4b0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the same evaluation queries\n",
    "eval_queries = [\n",
    "    \"What is reinforcement learning?\",\n",
    "    \"How does data visualization help in analysis?\",\n",
    "    \"Explain feature engineering in machine learning.\",\n",
    "    \"What is a large language model?\",\n",
    "    \"Describe data cleansing.\"\n",
    "]\n",
    "\n",
    "# Collect similarity scores\n",
    "heatmap_data = []\n",
    "\n",
    "for query in eval_queries:\n",
    "    _, retrieved = rag_answer(query, top_k=5)\n",
    "    scores = [r[\"similarity\"] for r in retrieved]\n",
    "    heatmap_data.append(scores)\n",
    "\n",
    "# Convert to numpy array\n",
    "heatmap_array = np.array(heatmap_data)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_array,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    xticklabels=[f\"Doc {i+1}\" for i in range(heatmap_array.shape[1])],\n",
    "    yticklabels=[f\"Q{i+1}\" for i in range(heatmap_array.shape[0])],\n",
    ")\n",
    "plt.title(\"RAG Retrieved Document Similarity Scores\")\n",
    "plt.xlabel(\"Retrieved Documents\")\n",
    "plt.ylabel(\"Queries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht73fTqhPDLa"
   },
   "source": [
    "## SUMMARY\n",
    "- Built a **RAG pipeline** to answer questions using a Wikipedia-based corpus on AI/ML/Data Science.\n",
    "- Steps covered:\n",
    "  - Created a **Wikipedia knowledge base** with topics and summaries.\n",
    "  - Converted the summaries into **embeddings** using a Transformer model.\n",
    "  - Implemented **cosine similarity-based retrieval** to fetch top-k relevant documents.\n",
    "  - Integrated retrieval results into a **Flan-T5 text generation pipeline**.\n",
    "  - Added **visualizations** (heatmap & PCA plots) to understand document similarity.\n",
    "- Key concepts:\n",
    "  - **Embeddings** → Dense vector representations of text.\n",
    "  - **Cosine Similarity** → Measures closeness between embeddings.\n",
    "  - **Top-k Retrieval** → Selects the most relevant documents for context.\n",
    "  - **RAG Workflow** → Retrieve → Combine Context → Generate Answer.\n",
    "- Tech stack:\n",
    "  - `transformers` (Hugging Face) for model & tokenizer\n",
    "  - `sentence-transformers` for embeddings\n",
    "  - `torch` for similarity & tensor operations\n",
    "  - `matplotlib` & `seaborn` for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aURVtd6QiPZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0202aec5376d478b9f930bfa3d296376": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2aae9e683a94ed5aa1bebe408aea3d6",
      "placeholder": "​",
      "style": "IPY_MODEL_59a0013f3398480882fcdd3ed5f646cc",
      "value": "Batches: 100%"
     }
    },
    "1f4d29359efa4bc1bb9f61ef7447fd4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a62e69d518f4df58e2f85d07b4d0cd3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e5a13e4593814d909c2f657cc47db74f",
      "value": 1
     }
    },
    "346b95f14515407a948e5b8a94c16eb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46e3cced7ddc490abb298a706ddcb5ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51deec0a7878438f81451d960d643d22",
      "placeholder": "​",
      "style": "IPY_MODEL_c135e39f87954861aa97dff48274ca7b",
      "value": " 1/1 [00:06&lt;00:00,  6.21s/it]"
     }
    },
    "4a62e69d518f4df58e2f85d07b4d0cd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51deec0a7878438f81451d960d643d22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59a0013f3398480882fcdd3ed5f646cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c135e39f87954861aa97dff48274ca7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df15e78d05494c87bb521a8722004762": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0202aec5376d478b9f930bfa3d296376",
       "IPY_MODEL_1f4d29359efa4bc1bb9f61ef7447fd4f",
       "IPY_MODEL_46e3cced7ddc490abb298a706ddcb5ca"
      ],
      "layout": "IPY_MODEL_346b95f14515407a948e5b8a94c16eb3"
     }
    },
    "e5a13e4593814d909c2f657cc47db74f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2aae9e683a94ed5aa1bebe408aea3d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
