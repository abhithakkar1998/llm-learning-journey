{
  "metadata": {
    "total_documents": 49,
    "topics_covered": [
      "Time",
      "Anomaly",
      "Retrieval-augmented",
      "Predictive",
      "Business",
      "Image",
      "Blockchain",
      "Machine",
      "Natural",
      "Data",
      "Object",
      "Artificial",
      "Neural",
      "Exploratory",
      "Large",
      "Statistical",
      "Deep",
      "ETL",
      "Cloud",
      "Generative",
      "Computer",
      "Quantum",
      "Face",
      "Reinforcement",
      "Feature"
    ],
    "generation_info": "Created by WikipediaDataLoader"
  },
  "documents": [
    {
      "id": "Artificial_Intelligence_0",
      "title": "Artificial intelligence",
      "content": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical opt...",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence"
    },
    {
      "id": "Artificial_Intelligence_1",
      "title": "Artificial general intelligence",
      "content": "Artificial general intelligence (AGI)—sometimes called human‑level intelligence AI—is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks.\nSome researchers argue that state‑of‑the‑art large language models (LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved. Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin.\nUnlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.\nCreating AGI is a primary goal of AI research and of companies such as OpenAI, Google, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\nThe timeline for achieving human‑level intelligence AI remains deeply contested. Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner—or never at all. There is debate on the exact ...",
      "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence"
    },
    {
      "id": "Machine_Learning_1",
      "title": "Neural network (machine learning)",
      "content": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks.\nA neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can le...",
      "url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)"
    },
    {
      "id": "Deep_Learning_1",
      "title": "Transformer (deep learning architecture)",
      "content": "In deep learning, transformer is a neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\n\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\n\n\n== History ==\n\n\n=== Predecessors ===\nFor m...",
      "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)"
    },
    {
      "id": "Neural_Networks_0",
      "title": "Neural network (machine learning)",
      "content": "In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks.\nA neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.\nArtificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can le...",
      "url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)"
    },
    {
      "id": "Neural_Networks_1",
      "title": "Net neutrality",
      "content": "Net neutrality, sometimes referred to as network neutrality, is the principle that Internet service providers (ISPs) must treat all Internet communications equally, offering users and online content providers consistent transfer rates regardless of content, website, platform, application, type of equipment, source address, destination address, or method of communication (i.e., without price discrimination). Net neutrality was advocated for in the 1990s by the presidential administration of Bill Clinton in the United States. Clinton signed the Telecommunications Act of 1996, an amendment to the Communications Act of 1934. In 2025, an American court ruled that Internet companies should not be regulated like utilities, which weakened net neutrality regulation and put the decision in the hands of the United States Congress and state legislatures.\nSupporters of net neutrality argue that it prevents ISPs from filtering Internet content without a court order, fosters freedom of speech and democratic participation, promotes competition and innovation, prevents dubious services, and maintains the end-to-end principle, and that users would be intolerant of slow-loading websites. Opponents argue that it reduces investment, deters competition, increases taxes, imposes unnecessary regulations, prevents the Internet from being accessible to lower income individuals, and prevents Internet traffic from being allocated to the most needed users, that large ISPs already have a performance advan...",
      "url": "https://en.wikipedia.org/wiki/Net_neutrality"
    },
    {
      "id": "Generative_AI_1",
      "title": "Generative AI pornography",
      "content": "Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\n\n\n== History ==\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI's release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI's warnings against sexual imagery, SD's public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\n\n\n=== AI-generated influencers ===\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but con...",
      "url": "https://en.wikipedia.org/wiki/Generative_AI_pornography"
    },
    {
      "id": "Computer_Vision_1",
      "title": "Computer vision dazzle",
      "content": "Computer vision dazzle, also known as CV dazzle, dazzle makeup, or anti-surveillance makeup, is a type of camouflage used to hamper facial recognition software, inspired by dazzle camouflage used by vehicles such as ships and planes. \n\n\n== Methods ==\nCV dazzle combines stylized makeup, asymmetric hair, and sometimes infrared lights built in to glasses or clothing to break up detectable facial patterns recognized by computer vision algorithms in much the same way that warships contrasted color and used sloping lines and curves to distort the structure of a vessel. \nIt has been shown to be somewhat successful at defeating face detection software in common use, including that employed by Facebook. CV dazzle attempts to block detection by facial recognition technologies such as DeepFace \"by creating an 'anti-face'\". It uses occlusion, covering certain facial features; transformation, altering the shape or colour of parts of the face; and a combination of the two. Prominent artists employing this technique include Adam Harvey and Jillian Mayer.\n\n\n== Use in protests ==\nComputer vision dazzle makeup has been used by rioters in several different protest movements. Its use as a protesting aid has often been found ineffective. It may be effective to thwart computer technology, but draws human attention, is easy for human monitors to spot on security cameras, and makes it hard for rioters to blend in within a crowd. Advances in facial recognition technology make dazzle makeup increasing...",
      "url": "https://en.wikipedia.org/wiki/Computer_vision_dazzle"
    },
    {
      "id": "Large_Language_Model_0",
      "title": "Large language model",
      "content": "A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), based on a transformer architecture, which are largely used in generative chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\n\n\n== History ==\n\nBefore the emergence of transformer-based models in 2017, some language models were considered large relative to the computational and data constraints of their time. In the early 1990s, IBM's statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. A smoothed n-gram model in 2001, such as those employing Kneser-Ney smoothing, trained on 300 million words achieved state-of-the-art perplexity on benchmark tests at the time. During the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web (\"web as corpus\") to train statistical language models.\n\nMoving beyond n-gram models, researchers started in 2000 to use neural networks to learn language models. Follo...",
      "url": "https://en.wikipedia.org/wiki/Large_language_model"
    },
    {
      "id": "Large_Language_Model_1",
      "title": "List of large language models",
      "content": "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\nThis page lists notable large language models.\n\n\n== List ==\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\n\n\n== See also ==\nList of chatbots\nList of language model benchmarks\n\n\n== Notes ==\n\n\n== References ==",
      "url": "https://en.wikipedia.org/wiki/List_of_large_language_models"
    },
    {
      "id": "Retrieval-augmented_generation_1",
      "title": "Prompt engineering",
      "content": "Prompt engineering is the process of structuring or crafting an instruction in order to produce better outputs from a generative artificial intelligence (AI) model.\nA prompt is natural language text describing the task that an AI should perform. A prompt for a text-to-text language model can be a query, a command, or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, choice of words and grammar, providing relevant context, or describing a character for the AI to mimic.\nWhen communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, or emphasizing words to achieve a desired subject, style, layout, lighting, and aesthetic.\n\n\n== History ==\nIn 2018, researchers first proposed that all previously separate tasks in natural language processing (NLP) could be cast as a question-answering problem over a context. In addition, they trained a first single, joint, multi-task model that would answer any task-related question like \"What is the sentiment\" or \"Translate this sentence to German\" or \"Who is the president?\"\nThe AI boom saw an increase in the amount of \"prompting technique\" to get the model to output the desired outcome and avoid nonsensical output, a pr...",
      "url": "https://en.wikipedia.org/wiki/Prompt_engineering"
    },
    {
      "id": "Object_Detection_1",
      "title": "You Only Look Once",
      "content": "You Only Look Once (YOLO) is a series of real-time object detection systems based on convolutional neural networks. First introduced by Joseph Redmon et al. in 2015, YOLO has undergone several iterations and improvements, becoming one of the most popular object detection frameworks.\nThe name \"You Only Look Once\" refers to the fact that the algorithm requires only one forward propagation pass through the neural network to make predictions, unlike previous region proposal-based techniques like R-CNN that require thousands for a single image.\n\n\n== Overview ==\nCompared to previous methods like R-CNN and OverFeat, instead of applying the model to an image at multiple locations and scales, YOLO applies a single neural network to the full image. This network divides the image into regions and predicts bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities.\n\n\n=== OverFeat ===\nOverFeat was an early influential model for simultaneous object classification and localization. Its architecture is as follows:\n\nTrain a neural network for image classification only (\"classification-trained network\"). This could be one like the AlexNet.\nThe last layer of the trained network is removed, and for every possible object class, initialize a network module at the last layer (\"regression network\"). The base network has its parameters frozen. The regression network is trained to predict the \n  \n    \n      \n        (\n        x\n        ,\n        ...",
      "url": "https://en.wikipedia.org/wiki/You_Only_Look_Once"
    },
    {
      "id": "Face_Recognition_0",
      "title": "Facial recognition system",
      "content": "A facial recognition system is a technology potentially capable of matching a human face from a digital image or a video frame against a database of faces. Such a system is typically employed to authenticate users through ID verification services, and works by pinpointing and measuring facial features from a given image.\nDevelopment began on similar systems in the 1960s, beginning as a form of computer application. Since their inception, facial recognition systems have seen wider uses in recent times on smartphones and in other forms of technology, such as robotics. Because computerized facial recognition involves the measurement of a human's physiological characteristics, facial recognition systems are categorized as biometrics. Although the accuracy of facial recognition systems as a biometric technology is lower than iris recognition, fingerprint image acquisition, palm recognition or voice recognition, it is widely adopted due to its contactless process. Facial recognition systems have been deployed in advanced human–computer interaction, video surveillance, law enforcement, passenger screening, decisions on employment and housing and automatic indexing of images.\nFacial recognition systems are employed throughout the world today by governments and private companies. Their effectiveness varies, and some systems have previously been scrapped because of their ineffectiveness. The use of facial recognition systems has also raised controversy, with claims that the systems vio...",
      "url": "https://en.wikipedia.org/wiki/Facial_recognition_system"
    },
    {
      "id": "Face_Recognition_1",
      "title": "Face perception",
      "content": "Facial perception is an individual's understanding and interpretation of the face. Here, perception implies the presence of consciousness and hence excludes automated facial recognition systems. Although facial recognition is found in other species, this article focuses on facial perception in humans.\nThe perception of facial features is an important part of social cognition. Information gathered from the face helps people understand each other's identity, what they are thinking and feeling, anticipate their actions, recognize their emotions, build connections, and communicate through body language. Developing facial recognition is a necessary building block for complex societal constructs. Being able to perceive identity, mood, age, sex, and race lets people mold the way we interact with one another, and understand our immediate surroundings.\nThough facial perception is mainly considered to stem from visual intake, studies have shown that even people born blind can learn face perception without vision. Studies have supported the notion of a specialized mechanism for perceiving faces.\n\n\n== Overview ==\nTheories about the processes involved in adult face perception have largely come from two sources; research on normal adult face perception and the study of impairments in face perception that are caused by brain injury or neurological illness.\n\n\n=== Bruce & Young model ===\n\nOne of the most widely accepted theories of face perception argues that understanding faces involves seve...",
      "url": "https://en.wikipedia.org/wiki/Face_perception"
    },
    {
      "id": "Natural_Language_Processing_1",
      "title": "Natural language",
      "content": "A natural language or ordinary language is a language that occurs organically in a human community by a process of use, repetition, and change and in forms such as written, spoken and signed. Categorization as natural includes languages associated with linguistic prescriptivism or language regulation, but excludes constructed and formal languages such as those used for computer programming and logic. Nonstandard dialects can be viewed as a wild type in comparison with standard languages. An official language with a regulating academy such as Standard French, overseen by the Académie Française, is classified as a natural language (e.g. in the field of natural language processing), as its prescriptive aspects do not make it constructed enough to be a constructed language or controlled enough to be a controlled natural language.\nCategorization as natural excludes:\n\nArtificial and constructed languages\nConstructed international auxiliary languages\nNon-human communication systems in nature such as whale and other marine mammal vocalizations or honey bees' waggle dance.\n\n\n== Controlled languages ==\n\nControlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce ambiguity and complexity. This may be accomplished by decreasing usage of superlative or adverbial forms, or irregular verbs. Typical purposes for developing and implementing a controlled natural language are to aid understanding by non-native speakers or...",
      "url": "https://en.wikipedia.org/wiki/Natural_language"
    },
    {
      "id": "Image_Processing_0",
      "title": "Digital image processing",
      "content": "Digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more), digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); and third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased.\n\n\n== History ==\n\nMany of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. The purpose of early image processing was to improve the quality of the image. It was aimed for human beings to impro...",
      "url": "https://en.wikipedia.org/wiki/Digital_image_processing"
    },
    {
      "id": "Image_Processing_1",
      "title": "Image processor",
      "content": "An image processor, also known as an image processing engine, image processing unit (IPU), or image signal processor (ISP), is a type of media processor or specialized digital signal processor (DSP) used for image processing, in digital cameras or other devices.\nImage processors often employ parallel computing even with SIMD or MIMD technologies to increase speed and efficiency. The digital image processing engine can perform a range of tasks. \nTo increase the system integration on embedded devices, often it is a system on a chip with multi-core processor architecture.\n\n\n== Function ==\n\n\n=== Bayer transformation ===\nThe photodiodes employed in an image sensor are color-blind by nature: they can only record shades of grey. To get color into the picture, they are covered with different color filters: red, green and blue (RGB) according to the pattern designated by the Bayer filter. As each photodiode records the color information for exactly one pixel of the image, without an image processor there would be a green pixel next to each red and blue pixel.\nThis process, however, is quite complex, and involves a number of different operations. Its quality depends largely on the effectiveness of the algorithms applied to the raw data coming from the sensor. The mathematically manipulated data becomes the recorded photo file.\n\n\n=== Demosaicing ===\nAs stated above, the image processor evaluates the color and brightness data of a given pixel, compares them with the data from neighboring...",
      "url": "https://en.wikipedia.org/wiki/Image_processor"
    },
    {
      "id": "Data_Science_0",
      "title": "Data science",
      "content": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data. \nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.\nData science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.\n\n\n== Foundations ==\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and...",
      "url": "https://en.wikipedia.org/wiki/Data_science"
    },
    {
      "id": "Data_Mining_1",
      "title": "Data stream mining",
      "content": "Data Stream Mining (also known as stream learning) is the process of extracting knowledge structures from continuous, rapid data records. A data stream is an ordered sequence of instances that in many applications of data stream mining can be read only once or a small number of times using limited computing and storage capabilities.\nIn many data stream mining applications, the goal is to predict the class or value of new instances in the data stream given some knowledge about the class membership or values of previous instances in the data stream.\nMachine learning techniques can be used to learn this prediction task from labeled examples in an automated fashion.\nOften, concepts from the field of incremental learning are applied to cope with structural changes, on-line learning and real-time demands. \nIn many applications, especially operating within non-stationary environments, the distribution underlying the instances or the rules underlying their labeling may change over time, i.e. the goal of the prediction, the class to be predicted or the target value to be predicted, may change over time. This problem is referred to as concept drift. Detecting concept drift is a central issue to data stream mining. Other challenges that arise when applying machine learning to streaming data include: partially and delayed labeled data, recovery from concept drifts, and temporal dependencies.\nExamples of data streams include computer network traffic, phone conversations, ATM transactions,...",
      "url": "https://en.wikipedia.org/wiki/Data_stream_mining"
    },
    {
      "id": "Data_Analytics_0",
      "title": "Analysis",
      "content": "Analysis (pl.: analyses) is the process of breaking a complex topic or substance into smaller parts in order to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384–322 BC), though analysis as a formal concept is a relatively recent development.\nThe word comes from the Ancient Greek ἀνάλυσις (analysis, \"a breaking-up\" or \"an untying\" from ana- \"up, throughout\" and lysis \"a loosening\"). From it also comes the word's plural, analyses.\nAs a formal concept, the method has variously been ascribed to René Descartes (Discourse on the Method), and Galileo Galilei. It has also been ascribed to Isaac Newton, in the form of a practical method of physical discovery (which he did not name).\nThe converse of analysis is synthesis: putting the pieces back together again in a new or different whole. \n\n\n== Science and technology ==\n\n\n=== Chemistry ===\n\nThe field of chemistry uses analysis in three ways: to identify the components of a particular chemical compound (qualitative analysis), to identify the proportions of components in a mixture (quantitative analysis), and to break down chemical processes and examine chemical reactions between elements of matter. For an example of its use, analysis of the concentration of elements is important in managing a nuclear reactor, so nuclear scientists will analyze neutron activation to develop discrete measurements within vast samples. A matrix can have a considerable effect ...",
      "url": "https://en.wikipedia.org/wiki/Analysis"
    },
    {
      "id": "Data_Analytics_1",
      "title": "Data analysis",
      "content": "Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.\nData mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a variety of unstructured data. All of the above are varieties of data analysis.\n\n\n== Data analysis process ==\n\nData analysis is a process for obtaining raw data, a...",
      "url": "https://en.wikipedia.org/wiki/Data_analysis"
    },
    {
      "id": "Predictive_Analytics_1",
      "title": "Analysis",
      "content": "Analysis (pl.: analyses) is the process of breaking a complex topic or substance into smaller parts in order to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384–322 BC), though analysis as a formal concept is a relatively recent development.\nThe word comes from the Ancient Greek ἀνάλυσις (analysis, \"a breaking-up\" or \"an untying\" from ana- \"up, throughout\" and lysis \"a loosening\"). From it also comes the word's plural, analyses.\nAs a formal concept, the method has variously been ascribed to René Descartes (Discourse on the Method), and Galileo Galilei. It has also been ascribed to Isaac Newton, in the form of a practical method of physical discovery (which he did not name).\nThe converse of analysis is synthesis: putting the pieces back together again in a new or different whole. \n\n\n== Science and technology ==\n\n\n=== Chemistry ===\n\nThe field of chemistry uses analysis in three ways: to identify the components of a particular chemical compound (qualitative analysis), to identify the proportions of components in a mixture (quantitative analysis), and to break down chemical processes and examine chemical reactions between elements of matter. For an example of its use, analysis of the concentration of elements is important in managing a nuclear reactor, so nuclear scientists will analyze neutron activation to develop discrete measurements within vast samples. A matrix can have a considerable effect ...",
      "url": "https://en.wikipedia.org/wiki/Analysis"
    },
    {
      "id": "Statistical_Modeling_0",
      "title": "Statistical model",
      "content": "A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process. When referring specifically to probabilities, the corresponding term is probabilistic model. All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference. A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables.  As such, a statistical model is \"a formal representation of a theory\" (Herman Adèr quoting Kenneth Bollen).\n\n\n== Introduction ==\nInformally, a statistical model can be thought of as a statistical assumption (or set of statistical assumptions) with a certain property: that the assumption allows us to calculate the probability of any event. As an example, consider a pair of ordinary six-sided dice. We will study two different statistical assumptions about the dice.\nThe first statistical assumption is this: for each of the dice, the probability of each face (1, 2, 3, 4, 5, and 6) coming up is ⁠1/6⁠. From that assumption, we can calculate the probability of both dice coming up 5:  ⁠1/6⁠ × ⁠1/6⁠ = ⁠1/36⁠.  More generally, we can calculate the probability of any event: e.g. (1 and 2) or (3 and 3)...",
      "url": "https://en.wikipedia.org/wiki/Statistical_model"
    },
    {
      "id": "Data_Visualization_0",
      "title": "Data and information visualization",
      "content": "Data and information visualization (data viz/vis or info viz/vis) is the practice of designing and creating graphic or visual representations of quantitative and qualitative data and information with the help of static, dynamic or interactive visual items. These visualizations are intended to help a target audience visually explore and discover, quickly understand, interpret and gain important insights into otherwise difficult-to-identify structures, relationships, correlations, local and global patterns, trends, variations, constancy, clusters, outliers and unusual groupings within data. When intended for the public to convey a concise version of information in an engaging manner, it is typically called infographics.  \nData visualization is concerned with presenting sets of primarily quantitative raw data in a schematic form, using imagery. The visual formats used in data visualization include charts and graphs, geospatial maps, figures, correlation matrices, percentage gauges, etc..\nInformation visualization deals with multiple, large-scale and complicated datasets which contain quantitative data, as well as qualitative, and primarily abstract information, and its goal is to add value to raw data, improve the viewers' comprehension, reinforce their cognition and help derive insights and make decisions as they navigate and interact with the graphical display. Visual tools used include maps for location based data; hierarchical organisations of data; displays that prioritise ...",
      "url": "https://en.wikipedia.org/wiki/Data_and_information_visualization"
    },
    {
      "id": "Data_Visualization_1",
      "title": "Biological data visualization",
      "content": "Biological data visualization is a branch of bioinformatics concerned with the application of computer graphics, scientific visualization, and information visualization to different areas of the life sciences. This includes visualization of sequences, genomes, alignments, phylogenies, macromolecular structures, systems biology, microscopy, and magnetic resonance imaging data. Software tools used for visualizing biological data range from simple, standalone programs to complex, integrated systems.\nAn emerging trend is the blurring of boundaries between the visualization of 3D structures at atomic resolution, the visualization of larger complexes by cryo-electron microscopy, and the visualization of the location of proteins and complexes within whole cells and tissues. There has also been an increase in the availability and importance of time-resolved data from systems biology, electron microscopy, and cell and tissue imaging.\n\n\n== Sequence alignment ==\n\nSequence alignment visualization plays a crucial role in bioinformatics and genomics by enabling researchers to interpret and analyze complex genetic data effectively. Visualizing sequence alignments allows for the identification of similarities, differences, conserved regions, and evolutionary patterns within DNA or protein sequences, aiding in understanding genetic relationships, functional elements, and evolutionary processes. Sequence alignment visualization is essential for several reasons:\nIdentifying conserved sequence: ...",
      "url": "https://en.wikipedia.org/wiki/Biological_data_visualization"
    },
    {
      "id": "Exploratory_Data_Analysis_0",
      "title": "Exploratory data analysis",
      "content": "In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell beyond the formal modeling and thereby contrasts with traditional hypothesis testing, in which a model is supposed to be selected before the data is seen. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.\n\n\n== Overview ==\nTukey defined data analysis in 1961 as: \"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"\nExploratory data analysis is a technique to analyze and investigate a dataset and summarize its main characteristics. A main advantage of EDA is providing the visualization of data after conducting analysis. \nTukey's championing of EDA encouraged th...",
      "url": "https://en.wikipedia.org/wiki/Exploratory_data_analysis"
    },
    {
      "id": "Exploratory_Data_Analysis_1",
      "title": "Data analysis",
      "content": "Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.\nData mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a variety of unstructured data. All of the above are varieties of data analysis.\n\n\n== Data analysis process ==\n\nData analysis is a process for obtaining raw data, a...",
      "url": "https://en.wikipedia.org/wiki/Data_analysis"
    },
    {
      "id": "Data_Cleaning_0",
      "title": "Data cleansing",
      "content": "Data cleansing or data cleaning is the process of identifying and correcting (or removing) corrupt, inaccurate, or irrelevant records from a dataset, table, or database. It involves detecting incomplete, incorrect, or inaccurate parts of the data and then replacing, modifying, or deleting the affected data. Data cleansing can be performed interactively using data wrangling tools, or through batch processing often via scripts or a data quality firewall.\nAfter cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.\nThe actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is m...",
      "url": "https://en.wikipedia.org/wiki/Data_cleansing"
    },
    {
      "id": "Data_Cleaning_1",
      "title": "Data analysis",
      "content": "Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.\nData mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a variety of unstructured data. All of the above are varieties of data analysis.\n\n\n== Data analysis process ==\n\nData analysis is a process for obtaining raw data, a...",
      "url": "https://en.wikipedia.org/wiki/Data_analysis"
    },
    {
      "id": "ETL_(Extract_Transform_Load)_0",
      "title": "Extract, transform, load",
      "content": "Extract, transform, load (ETL) is a three-phase computing process where data is extracted from an input source, transformed (including cleaning), and loaded into an output data container. The data can be collected from one or more sources and it can also be output to one or more destinations. ETL processing is typically executed using software applications but it can also be done manually by system operators. ETL software typically automates the entire process and can be run manually or on recurring schedules either as single jobs or aggregated into a batch of jobs.\nA properly designed ETL system extracts data from source systems and enforces data type and data validity standards and ensures it conforms structurally to the requirements of the output. Some ETL systems can also deliver data in a presentation-ready format so that application developers can build applications and end users can make decisions.\nThe ETL process is often used in data warehousing. ETL systems commonly integrate data from multiple applications (systems), typically developed and supported by different vendors or hosted on separate computer hardware. The separate systems containing the original data are frequently managed and operated by different stakeholders. For example, a cost accounting system may combine data from payroll, sales, and purchasing.\nData extraction involves extracting data from homogeneous or heterogeneous sources; data transformation processes data by data cleaning and transforming it...",
      "url": "https://en.wikipedia.org/wiki/Extract,_transform,_load"
    },
    {
      "id": "ETL_(Extract_Transform_Load)_1",
      "title": "Extract, transform, load",
      "content": "Extract, transform, load (ETL) is a three-phase computing process where data is extracted from an input source, transformed (including cleaning), and loaded into an output data container. The data can be collected from one or more sources and it can also be output to one or more destinations. ETL processing is typically executed using software applications but it can also be done manually by system operators. ETL software typically automates the entire process and can be run manually or on recurring schedules either as single jobs or aggregated into a batch of jobs.\nA properly designed ETL system extracts data from source systems and enforces data type and data validity standards and ensures it conforms structurally to the requirements of the output. Some ETL systems can also deliver data in a presentation-ready format so that application developers can build applications and end users can make decisions.\nThe ETL process is often used in data warehousing. ETL systems commonly integrate data from multiple applications (systems), typically developed and supported by different vendors or hosted on separate computer hardware. The separate systems containing the original data are frequently managed and operated by different stakeholders. For example, a cost accounting system may combine data from payroll, sales, and purchasing.\nData extraction involves extracting data from homogeneous or heterogeneous sources; data transformation processes data by data cleaning and transforming it...",
      "url": "https://en.wikipedia.org/wiki/Extract,_transform,_load"
    },
    {
      "id": "Business_Intelligence_0",
      "title": "Business intelligence",
      "content": "Business intelligence (BI) consists of strategies, methodologies, and technologies used by enterprises for data analysis and management of business information to inform business strategies and business operations.  Common functions of BI technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. \nBI tools can handle large amounts of structured and sometimes unstructured data to help organizations identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights is assumed to potentially provide businesses with a competitive market advantage and long-term stability, and help them take strategic decisions.\nBusiness intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, Business Intelligence (BI) is considered most effective when it combines data from the market in which a company operates (external data) with data from internal company sources, such as financial and operational ...",
      "url": "https://en.wikipedia.org/wiki/Business_intelligence"
    },
    {
      "id": "Business_Intelligence_1",
      "title": "Business intelligence software",
      "content": "Business intelligence software is a type of application software designed to retrieve, analyze, transform and report data for business intelligence (BI).  The applications generally read data that has been previously stored, often - though not necessarily - in a data warehouse or data mart.\n\n\n== History ==\n\n\n=== Development of business intelligence software ===\nThe first comprehensive business intelligence systems were developed by IBM and Siebel (currently acquired by Oracle) in the period between 1970 and 1990. At the same time, small developer teams were emerging with attractive ideas, and pushing out some of the products companies still use nowadays.\nIn 1988, specialists and vendors organized a Multiway Data Analysis Consortium in Rome, where they considered making data management and analytics more efficient, and foremost available to smaller and financially restricted businesses. By 2000, there were many professional reporting systems and analytic programs, some owned by top performing software producers in the United States of America.\n\n\n=== Cloud-hosted business intelligence software ===\nIn the years after 2000, business intelligence software producers became interested in producing universally applicable BI systems which don’t require expensive installation, and could hence be considered by smaller and midmarket businesses which could not afford on premise maintenance. These aspirations emerged in parallel with the cloud hosting trend, which is how most vendors came ...",
      "url": "https://en.wikipedia.org/wiki/Business_intelligence_software"
    },
    {
      "id": "Data_Warehousing_1",
      "title": "Dimension (data warehouse)",
      "content": "A dimension is a structure that categorizes facts and measures in order to enable users to answer business questions. Commonly used dimensions are people, products, place and time. (Note: People and time sometimes are not modeled as dimensions.)\nIn a data warehouse, dimensions provide structured labeling information to otherwise unordered numeric measures. The dimension is a data set composed of individual, non-overlapping data elements. The primary functions of dimensions are threefold: to provide filtering, grouping and labelling.\nThese functions are often described as \"slice and dice\". A common data warehouse example involves sales as the measure, with customer and product as dimensions. In each sale a customer buys a product. The data can be sliced by removing all customers except for a group under study, and then diced by grouping by product.\nA dimensional data element is similar to a categorical variable in statistics.\nTypically dimensions in a data warehouse are organized internally into one or more hierarchies. \"Date\" is a common dimension, with several possible hierarchies:\n\n\"Days (are grouped into) Months (which are grouped into) Years\",\n\"Days (are grouped into) Weeks (which are grouped into) Years\"\n\"Days (are grouped into) Months (which are grouped into) Quarters (which are grouped into) Years\"\netc.\n\n\n== Types ==\n\n\n=== Slowly changing dimensions ===\nA slowly changing dimension is a set of data attributes that change slowly over a period of time rather than changing...",
      "url": "https://en.wikipedia.org/wiki/Dimension_(data_warehouse)"
    },
    {
      "id": "Feature_Engineering_0",
      "title": "Feature engineering",
      "content": "Feature engineering is a preprocessing step in supervised machine learning and statistical modeling which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability. \nBeyond machine learning, the principles of feature engineering are applied in various scientific fields, including physics. For example, physicists construct dimensionless numbers such as the Reynolds number in fluid dynamics, the Nusselt number in heat transfer, and the Archimedes number in sedimentation. They also develop first approximations of solutions, such as analytical solutions for the strength of materials in mechanics.\n\n\n== Clustering ==\nOne of the applications of feature engineering has been clustering of feature-objects or sample-objects in a dataset. Especially, feature engineering based on matrix decomposition has been extensively used for data clustering under non-negativity constraints on the feature coefficients. These include Non-Negative Matrix Factorization (NMF), Non-Negative Matrix-Tri Factorization (NMTF), Non-Negative Tensor Decomposition/Factorization (NTF/NTD), etc. The non-negativity constraints on coefficients of the feature vectors mined by the above-stated algorithms yields a part-based representation, and different factor matrices exhibit natural clustering properties. Several ex...",
      "url": "https://en.wikipedia.org/wiki/Feature_engineering"
    },
    {
      "id": "Feature_Engineering_1",
      "title": "Feature (machine learning)",
      "content": "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a data set. Choosing informative, discriminating, and independent features is crucial to produce effective algorithms for pattern recognition, classification, and regression tasks. Features are usually numeric, but other types such as strings and graphs are used in syntactic pattern recognition, after some pre-processing step such as one-hot encoding. The concept of \"features\" is related to that of explanatory variables used in statistical techniques such as linear regression.\n\n\n== Feature types ==\nIn feature engineering, two types of features are commonly used: numerical and categorical.\nNumerical features are continuous values that can be measured on a scale. Examples of numerical features include age, height, weight, and income. Numerical features can be used in machine learning algorithms directly.\nCategorical features are discrete values that can be grouped into categories. Examples of categorical features include gender, color, and zip code. Categorical features typically need to be converted to numerical features before they can be used in machine learning algorithms. This can be done using a variety of techniques, such as one-hot encoding, label encoding, and ordinal encoding.\nThe type of feature that is used in feature engineering depends on the specific machine learning algorithm that is being used. Some machine learning algorithms, such as decision trees...",
      "url": "https://en.wikipedia.org/wiki/Feature_(machine_learning)"
    },
    {
      "id": "Time_Series_Analysis_0",
      "title": "Time series",
      "content": "In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order.  Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\nA time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.\nTime series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. Generally, time series data is modelled as a stochastic process. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called \"time series analysis\", which refers in particular to relationships between different points in time within a single series.\nTime series data have a natural temporal ordering.  This makes time ser...",
      "url": "https://en.wikipedia.org/wiki/Time_series"
    },
    {
      "id": "Time_Series_Analysis_1",
      "title": "Interrupted time series",
      "content": "Interrupted time series analysis (ITS), sometimes known as quasi-experimental time series analysis, is a method of statistical analysis involving tracking a long-term period before and after a point of intervention to assess the intervention's effects. The time series refers to the data over the period, while the interruption is the intervention, which is a controlled external influence or set of influences. Effects of the intervention are evaluated by changes in the level and slope of the time series and statistical significance of the intervention parameters. Interrupted time series design is the design of experiments based on the interrupted time series approach.\nThe method is used in various areas of research, such as:\n\npolitical science: impact of changes in laws on the behavior of people; (e.g., Effectiveness of sex offender registration policies in the United States)\neconomics: impact of changes in credit controls on borrowing behavior;\nsociology: impact of experiments in income maintenance on the behavior of participants in welfare programs;\nhistory: impact of major historical events on the behavior of those affected by the events;\npsychology: impact of expressing emotional experiences on online content;\nmedicine: in medical research, medical treatment is an intervention whose effect are to be studied;\nmarketing research: to analyze the effect of \"designed market interventions\" (e.g., advertising) on sales.\nenvironmental sciences: impacts of human activities on enviro...",
      "url": "https://en.wikipedia.org/wiki/Interrupted_time_series"
    },
    {
      "id": "Reinforcement_Learning_0",
      "title": "Reinforcement learning",
      "content": "Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nReinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed). The search for this balance is known as the exploration–exploitation dilemma.\n\nThe environment is typically stated in the form of a Markov decision process, as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large Markov decision processes where exact methods become infeasible. \n\n\n== Principles ==\nDue to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based opti...",
      "url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
    },
    {
      "id": "Reinforcement_Learning_1",
      "title": "Deep reinforcement learning",
      "content": "Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. RL considers the problem of a computational agent learning to make decisions by trial and error. Deep RL incorporates deep learning into the solution, allowing agents to make decisions from unstructured input data without manual engineering of the state space. Deep RL algorithms are able to take in very large inputs (e.g. every pixel rendered to the screen in a video game) and decide what actions to perform to optimize an objective (e.g. maximizing the game score). Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare.\n\n\n== Overview ==\n\n\n=== Deep learning ===\n\nDeep learning is a form of machine learning that utilizes a neural network to transform a set of inputs into a set of outputs via an artificial neural network. Deep learning methods, often using supervised learning with labeled datasets, have been shown to solve tasks that involve handling complex, high-dimensional raw input data (such as images) with less manual feature engineering than prior methods, enabling significant progress in several fields including computer vision and natural language processing. In the past decade, deep RL has achieved remarkable results on a range of problems, from single and multiplayer games...",
      "url": "https://en.wikipedia.org/wiki/Deep_reinforcement_learning"
    },
    {
      "id": "Anomaly_Detection_1",
      "title": "Intrusion detection system",
      "content": "An intrusion detection system (IDS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any intrusion activity or violation is typically either reported to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.\nIDS types range in scope from single computers to large networks. The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS). A system that monitors important operating system files is an example of an HIDS, while a system that analyzes incoming network traffic is an example of an NIDS. It is also possible to classify IDS by detection approach. The most well-known variants are signature-based detection (recognizing bad patterns, such as exploitation attempts) and anomaly-based detection (detecting deviations from a model of \"good\" traffic, which often relies on machine learning). Another common variant is reputation-based detection (recognizing the potential threat according to the reputation scores). Some IDS products have the ability to respond to detected intrusions. Systems with response capabilities are typically referred to as an intrusion prevention system (IPS). Intrusion detection systems can also serve specific purposes by augmenting them ...",
      "url": "https://en.wikipedia.org/wiki/Intrusion_detection_system"
    },
    {
      "id": "Data_Governance_0",
      "title": "Data governance",
      "content": "Data governance is a term used on both a macro and a micro level. The former is a political concept and forms part of international relations and Internet governance; the latter is a data management concept and forms part of corporate/organisational data governance.\nData governance involves delegating authority over data and exercising that authority through decision-making processes. It plays a crucial role in enhancing the value of data assets.\n\n\n== Macro level ==\nData governance at the macro level involves regulating cross-border data flows among countries, which is more precisely termed international data governance. This field formed in the early 2000s and consists of \"norms, principles and rules governing various types of data.\"\nThere have been several international groups established by research organizations that aim to grant access to their data. These groups that enable an exchange of data are, as a result, exposed to domestic and international legal interpretations that ultimately decide how data is used. However, as of 2023, there are no international laws or agreements specifically focused on data protection.\n\n\n== Micro level ==\nData governance in an organisational/corporate sense, is the capability that enables an organization to manage data effectively, securely, and responsibly. Data governance is the policies, processes, roles, responsibilities, and technologies in place to ensure that the right entities have access to accurate, complete, high quality data. T...",
      "url": "https://en.wikipedia.org/wiki/Data_governance"
    },
    {
      "id": "Data_Governance_1",
      "title": "Data governance",
      "content": "Data governance is a term used on both a macro and a micro level. The former is a political concept and forms part of international relations and Internet governance; the latter is a data management concept and forms part of corporate/organisational data governance.\nData governance involves delegating authority over data and exercising that authority through decision-making processes. It plays a crucial role in enhancing the value of data assets.\n\n\n== Macro level ==\nData governance at the macro level involves regulating cross-border data flows among countries, which is more precisely termed international data governance. This field formed in the early 2000s and consists of \"norms, principles and rules governing various types of data.\"\nThere have been several international groups established by research organizations that aim to grant access to their data. These groups that enable an exchange of data are, as a result, exposed to domestic and international legal interpretations that ultimately decide how data is used. However, as of 2023, there are no international laws or agreements specifically focused on data protection.\n\n\n== Micro level ==\nData governance in an organisational/corporate sense, is the capability that enables an organization to manage data effectively, securely, and responsibly. Data governance is the policies, processes, roles, responsibilities, and technologies in place to ensure that the right entities have access to accurate, complete, high quality data. T...",
      "url": "https://en.wikipedia.org/wiki/Data_governance"
    },
    {
      "id": "Data_Ethics_0",
      "title": "Big data ethics",
      "content": "Big data ethics, also known simply as data ethics, refers to systemizing, defending, and recommending concepts of right and wrong conduct in relation to data, in particular personal data. Since the dawn of the Internet the sheer quantity and quality of data has dramatically increased and is continuing to do so exponentially. Big data describes this large amount of data that is so voluminous and complex that traditional data processing application software is inadequate to deal with them. Recent innovations in medical research and healthcare, such as high-throughput genome sequencing, high-resolution imaging, electronic medical patient records and a plethora of internet-connected health devices have triggered a data deluge that will reach the exabyte range in the near future. Data ethics is of increasing relevance as the quantity of data increases because of the scale of the impact.\nBig data ethics are different from information ethics because the focus of information ethics is more concerned with issues of intellectual property and concerns relating to librarians, archivists, and information professionals, while big data ethics is more concerned with collectors and disseminators of structured or unstructured data such as data brokers, governments, and large corporations.  However, since artificial intelligence or machine learning systems are regularly built using big data sets, the discussions surrounding data ethics are often intertwined with those in the ethics of artificia...",
      "url": "https://en.wikipedia.org/wiki/Big_data_ethics"
    },
    {
      "id": "Data_Ethics_1",
      "title": "Data science",
      "content": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data. \nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.\nData science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.\n\n\n== Foundations ==\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and...",
      "url": "https://en.wikipedia.org/wiki/Data_science"
    },
    {
      "id": "Cloud_Computing_for_Data_Science_0",
      "title": "Data science",
      "content": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data. \nData science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.\nData science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\nA data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.\n\n\n== Foundations ==\nData science is an interdisciplinary field focused on extracting knowledge from typically large data sets and...",
      "url": "https://en.wikipedia.org/wiki/Data_science"
    },
    {
      "id": "Quantum_Computing_1",
      "title": "Timeline of quantum computing and communication",
      "content": "This is a timeline of quantum computing and communication.\n\n\n== 1960s ==\n\n\n=== 1968/69/70 ===\nStephen Wiesner invents conjugate coding.\n\n\n=== 1969 ===\n13 June – James L. Park (Washington State University, Pullman)'s paper is received by Foundations of Physics, in which he describes the non possibility of disturbance in a quantum transition state in the context of a disproof of quantum jumps in the concept of the atom described by Bohr.\n\n\n== 1970s ==\n\n\n=== 1973 ===\nAlexander Holevo's paper is published. The Holevo bound describes a limit of the quantity of classical information which is possible to quanta encode.\nCharles H. Bennett shows that computation can be done reversibly.\n\n\n=== 1975 ===\nR. P. Poplavskii publishes \"Thermodynamical models of information processing\" (in Russian) which shows the computational infeasibility of simulating quantum systems on classical computers, due to the superposition principle.\nRoman Stanisław Ingarden, a Polish mathematical physicist, submits the paper \"Quantum Information Theory\" in Reports on Mathematical Physics, vol. 10, pp. 43–72, published 1976. It is one of the first attempts at creating a quantum information theory, showing that Shannon information theory cannot directly be generalized to the quantum case, but rather that it is possible to construct a quantum information theory, which is a generalization of Shannon's theory, within the formalism of a generalized quantum mechanics of open systems and a generalized concept of observab...",
      "url": "https://en.wikipedia.org/wiki/Timeline_of_quantum_computing_and_communication"
    },
    {
      "id": "Blockchain_Technology_0",
      "title": "Blockchain",
      "content": "The blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leaves). Since each block contains information about the previous block, they effectively form a chain (compare linked list data structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are resistant to alteration because, once recorded, the data in any given block cannot be changed retroactively without altering all subsequent blocks and obtaining network consensus to accept these changes.\nBlockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance.\nA blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The implementation of the bloc...",
      "url": "https://en.wikipedia.org/wiki/Blockchain"
    },
    {
      "id": "Blockchain_Technology_1",
      "title": "Blockchain game",
      "content": "Video games can include elements that use blockchain technologies, including cryptocurrencies and non-fungible tokens (NFTs), often as a form of monetization. These elements typically allow players to trade in-game items for cryptocurrency, or represent in-game items with NFTs. Blockchain games have existed since 2017, gaining wider attention from the video game industry in 2021, when several AAA publishers expressed an intent to include this technology in the future. Players, developers, and game companies have criticized the use of blockchain technology in video games for being exploitative, environmentally unsustainable, and unnecessary.\nA subset of these games are also known as play-to-earn games because they include systems that allow players to earn cryptocurrency through gameplay.\n\n\n== Concept ==\nBlockchain technology, such as cryptocurrencies and NFTs, provides potential monetization routes for video games. Many live-service games offer in-game customization options, such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling. This has led to gray market issues such as skin gambling, and so publishers typically have shied away from allowing players to earn real-world funds from games. Blockchain games typically allow players to trade in-...",
      "url": "https://en.wikipedia.org/wiki/Blockchain_game"
    }
  ]
}